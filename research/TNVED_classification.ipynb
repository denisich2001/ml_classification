{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7a424c",
   "metadata": {},
   "source": [
    "## Замечания:\n",
    "Актуально:\n",
    "* С дисбалансом из-за полупустых классов можно бороться либо удалением маленьких классов, либо **пересемплированием**. Пока выбираю второй вариант, но в любом случае чудес ждать от предсказания маленьких по объему классов не стоит - если в обучающей выборке есть 1 строка какого-то класса, значит он будет предсказывать этот класс исключительно при полном соответствии тестовых строк данной одной строке. \n",
    "*\n",
    "\n",
    "\n",
    "Актуально для меня:\n",
    "* Можно улучшить обработку текстовых данных (можно попробовать в сыром виде запихать в кэтбуст)\n",
    "* Если лемматизация будет работать долго (есть такая вероятность) - можно поменять на стеммер\n",
    "\n",
    "Старое:\n",
    "* Время работы? - **нужно протестировать на больших датасетах**\n",
    "* Полупустые классы из-за которых возникает дисбаланс - **бороться**\n",
    "\n",
    "* Вариант с тем, что в одной колонке название характеристики, в другой значение будет работать плохо (вот пример), т.к. модели без разницы на порядок следования колонок\n",
    "* Правильно ли я понимаю, что все колонки кроме ХК 1 и целевой имеют тип данных String?\n",
    "* **Как предсказывать строки с пустыми значениями во всех колонках ХК? (может их сразу откидывать)?**\n",
    "* Названия первой колонки должны быть всегда одинаковые\n",
    "* !!!Важно!!! Будем заменять числовые факторы на категориальные, если в них маленькое количество уникальных значений или одно значение встречается очень часто\n",
    "* Были ошибки в названиях колонок: 'ХК_ка т_01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d3e75-a660-479c-a05e-2dab064987f0",
   "metadata": {},
   "source": [
    "**Проблемы, решение которых нужно будет автоматизировать:**\n",
    "* Несбалансированность классов\n",
    "* Пропуски в данных\n",
    "* Автоматическое кодирование текстовых столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "10e9e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install loguru\n",
    "#!pip install imblearn\n",
    "#!pip install pymystem3\n",
    "#!pip install catboost\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04316d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a94931",
   "metadata": {},
   "source": [
    "#### Глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6121d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Максимальное количество уникальных значений для категориального фактора, при котором он может обрабатываться методом one-hot encoding (добавится максимум столько столбцов)\n",
    "OneHotEncodingLimit = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b4d29",
   "metadata": {},
   "source": [
    "### Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d87f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/tire_classificator_data.xlsx')\n",
    "#df = pd.read_excel('data/paper_classificator_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac541b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop('Историческое наименование', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e945fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ID класса (ТАРГЕТ)'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81bd7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID класса (ТАРГЕТ)</th>\n",
       "      <th>Наименование терминального класса</th>\n",
       "      <th>Код родительского класса</th>\n",
       "      <th>Наименование родительского класса</th>\n",
       "      <th>Историческое наименование</th>\n",
       "      <th>ХК_Кат_01</th>\n",
       "      <th>Значение ХК_Кат_01</th>\n",
       "      <th>ХК_Кат_02</th>\n",
       "      <th>Значение ХК_Кат_02</th>\n",
       "      <th>ХК_Кат_03</th>\n",
       "      <th>Значение ХК_Кат_03</th>\n",
       "      <th>ХК_Кат_04</th>\n",
       "      <th>Значение ХК_Кат_04</th>\n",
       "      <th>ХК_Кат_05</th>\n",
       "      <th>Значение ХК_Кат_05</th>\n",
       "      <th>ХК_Стр_01</th>\n",
       "      <th>Значение ХК_Стр_01</th>\n",
       "      <th>ХК_Булево_01</th>\n",
       "      <th>Значение ХК_Булево_01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12388594</td>\n",
       "      <td>Шины автомобильные зимние 215/75 R16</td>\n",
       "      <td>01.09.08.01</td>\n",
       "      <td>ШИНЫ АВТОМОБИЛЬНЫЕ</td>\n",
       "      <td>Шины автомобильные зимние 215/75, R16, 116/114...</td>\n",
       "      <td>Производитель</td>\n",
       "      <td>GoodYear</td>\n",
       "      <td>Диаметр</td>\n",
       "      <td>R16</td>\n",
       "      <td>Размерность</td>\n",
       "      <td>215/75</td>\n",
       "      <td>Сезоность</td>\n",
       "      <td>зимние</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID класса (ТАРГЕТ)     Наименование терминального класса  \\\n",
       "0            12388594  Шины автомобильные зимние 215/75 R16   \n",
       "\n",
       "  Код родительского класса Наименование родительского класса  \\\n",
       "0              01.09.08.01                ШИНЫ АВТОМОБИЛЬНЫЕ   \n",
       "\n",
       "                           Историческое наименование      ХК_Кат_01  \\\n",
       "0  Шины автомобильные зимние 215/75, R16, 116/114...  Производитель   \n",
       "\n",
       "  Значение ХК_Кат_01 ХК_Кат_02 Значение ХК_Кат_02    ХК_Кат_03  \\\n",
       "0           GoodYear   Диаметр                R16  Размерность   \n",
       "\n",
       "  Значение ХК_Кат_03  ХК_Кат_04 Значение ХК_Кат_04 ХК_Кат_05  \\\n",
       "0             215/75  Сезоность             зимние       NaN   \n",
       "\n",
       "  Значение ХК_Кат_05 ХК_Стр_01 Значение ХК_Стр_01 ХК_Булево_01  \\\n",
       "0                NaN       NaN                NaN          NaN   \n",
       "\n",
       "  Значение ХК_Булево_01  \n",
       "0                   NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9af1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['ID класса (ТАРГЕТ)']\n",
    "\n",
    "# Удалим все лишние текстовые столбцы кроме \"Исторического наименования\"\n",
    "trainset_columns = []\n",
    "for column in df.columns:\n",
    "    if (column == 'Историческое наименование') or (re.fullmatch(r'ХК_.*', column)!=None) or (re.fullmatch(r'Значение.*', column)!=None):\n",
    "        trainset_columns.append(column)\n",
    "\n",
    "factors_df = df[trainset_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2355b",
   "metadata": {},
   "source": [
    "#### Работаем с типами данных столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cee265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_column_types(columns: list):\n",
    "    '''\n",
    "    Обрабатывает названия колонок из массива columns.\n",
    "    Возвращает словарь с парами: название колонки - ее тип данных  \n",
    "    '''\n",
    "    feature_types_dict = {}\n",
    "    for column in columns:\n",
    "        type_pattern = r'ХК_([^_]+)_.*'\n",
    "        if column[0:2] == 'ХК':\n",
    "            feature_types_dict[column] = 'Кат'\n",
    "        elif column[0:8]=='Значение':\n",
    "            column_type = re.findall(type_pattern, column)[0]\n",
    "            feature_types_dict[column] = column_type\n",
    "        else:\n",
    "            feature_types_dict[column] = 'Стр'\n",
    "    return feature_types_dict\n",
    "\n",
    "feature_types_dict = format_column_types(factors_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79196d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16664\\1022614043.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  factors_df[feature] = factors_df_copy[feature].astype(object)\n",
      "2023-10-22 23:46:06.919 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_01\n",
      "2023-10-22 23:46:06.921 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_01\n",
      "2023-10-22 23:46:06.924 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_02\n",
      "2023-10-22 23:46:06.928 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_02\n",
      "2023-10-22 23:46:06.931 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_03\n",
      "2023-10-22 23:46:06.932 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_03\n",
      "2023-10-22 23:46:06.933 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_04\n",
      "2023-10-22 23:46:06.934 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_04\n",
      "2023-10-22 23:46:06.935 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_05\n",
      "2023-10-22 23:46:06.936 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_05\n",
      "2023-10-22 23:46:06.937 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Стр_01\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16664\\1022614043.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  factors_df[feature] = factors_df_copy[feature].astype(object)\n",
      "2023-10-22 23:46:06.949 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Булево_01\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16664\\1022614043.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  factors_df[feature] = factors_df_copy[feature].astype(bool)\n"
     ]
    }
   ],
   "source": [
    "def check_number_to_categorical(column: str, factor: pd.Series):\n",
    "    logger.info(f'Начинаем проверку численного фактора {column}\\n')\n",
    "    logger.debug(f'Размер фактора:{factor.size}')\n",
    "    logger.debug(f'Количество уникальных значений: {factor.drop_duplicates().size}')\n",
    "    logger.debug(f'Процент заполненности фактора: {factor[factor.notnull()].size / factor.size * 100}%')\n",
    "    popular_value = pd.DataFrame(factor.value_counts().sort_values(ascending=False).head(1)/factor[factor.notnull()].size*100)\n",
    "    popular_value.columns = ['Частота']\n",
    "    logger.debug(f'Cамое частое значение фактора: \\n{popular_value}')\n",
    "    \n",
    "    if float(popular_value.iloc[0])>=50:\n",
    "        logger.info(f'Переводим числовой фактор {column} в категориальный')\n",
    "        return True\n",
    "    \n",
    "    \n",
    "for feature in feature_types_dict.keys():\n",
    "    if feature_types_dict.get(feature) == 'Стр':\n",
    "        #logger.debug(f'Строковый фактор: {feature}')\n",
    "        #todo ДОДЕЛАТЬ пока ничего не делаем, чтобы не потерять пропущенные значения при преобразовании в строковый формат\n",
    "        factors_df_copy = factors_df.copy()\n",
    "        factors_df[feature] = factors_df_copy[feature].astype(object)\n",
    "    elif feature_types_dict.get(feature) == 'Булево':\n",
    "        factors_df_copy = factors_df.copy()\n",
    "        factors_df[feature] = factors_df_copy[feature].astype(bool)\n",
    "    elif feature_types_dict.get(feature) == 'Числ':\n",
    "        if check_number_to_categorical(feature, factors_df[feature]):\n",
    "            feature_types_dict[feature] = 'Кат'\n",
    "            factors_df_copy = factors_df.copy()\n",
    "            factors_df[feature] = factors_df_copy[feature].astype(object)\n",
    "        else:\n",
    "            factors_df_copy = factors_df.copy()\n",
    "            factors_df[feature] = factors_df_copy[feature].astype(float)\n",
    "    elif feature_types_dict.get(feature) == 'Кат':\n",
    "        logger.debug(f'Категориальный фактор: {feature}')\n",
    "        #todo ДОДЕЛАТЬ преобразование категориальных колонок (пока не делаем, т.к. возможно будет catboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5a9e5",
   "metadata": {},
   "source": [
    "#### Заполняем пропуски в данных\n",
    "В зависимости от типа данных колонки заполняем пропуски по-разному:\n",
    "*   Стр -  т.к. переводим строки в числа, то пропущенные значение пусть будут = 0\n",
    "*   Числ - #todo По умолчанию = 0. Если присутствует значение, количество которого в заполненных строках >=50% => то фактор станет категориальным, а не численным. \n",
    "*   Булево - #todo будем считать, что у нас всегда такие столбцы отвечают на вопрос: \"Есть что-то? - Да/Нет\". Если нет ответа => Нет\n",
    "*   Кат - 'Emptyclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af29dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Историческое наименование     0\n",
       "ХК_Кат_01                     0\n",
       "Значение ХК_Кат_01            0\n",
       "ХК_Кат_02                     0\n",
       "Значение ХК_Кат_02            0\n",
       "ХК_Кат_03                     0\n",
       "Значение ХК_Кат_03            0\n",
       "ХК_Кат_04                     0\n",
       "Значение ХК_Кат_04            0\n",
       "ХК_Кат_05                    40\n",
       "Значение ХК_Кат_05           40\n",
       "ХК_Стр_01                    49\n",
       "Значение ХК_Стр_01           49\n",
       "ХК_Булево_01                 40\n",
       "Значение ХК_Булево_01         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fcd845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_empty_factors_df = factors_df.copy()\n",
    "for column in not_empty_factors_df.columns:\n",
    "    if feature_types_dict.get(column) == 'Кат':\n",
    "        not_empty_factors_df.loc[not_empty_factors_df[column].isna(), column] = f'EmptyCat'\n",
    "    elif feature_types_dict.get(column) == 'Числ':\n",
    "        not_empty_factors_df.loc[not_empty_factors_df[column].isna(), column] = 0\n",
    "    elif feature_types_dict.get(column) == 'Стр':\n",
    "        not_empty_factors_df.loc[not_empty_factors_df[column].isna(), column] = ''\n",
    "    elif feature_types_dict.get(column) == 'Булево':\n",
    "        not_empty_factors_df.loc[not_empty_factors_df[column].isna(), column] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06b664ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Историческое наименование    9151\n",
       "ХК_Кат_01                    9151\n",
       "Значение ХК_Кат_01           9151\n",
       "ХК_Кат_02                    9151\n",
       "Значение ХК_Кат_02           9151\n",
       "ХК_Кат_03                    9151\n",
       "Значение ХК_Кат_03           9151\n",
       "ХК_Кат_04                    9151\n",
       "Значение ХК_Кат_04           9151\n",
       "ХК_Кат_05                    9151\n",
       "Значение ХК_Кат_05           9151\n",
       "ХК_Стр_01                    9151\n",
       "Значение ХК_Стр_01           9151\n",
       "ХК_Булево_01                 9151\n",
       "Значение ХК_Булево_01        9151\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_empty_factors_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ae2be",
   "metadata": {},
   "source": [
    "#### Кодируем переменные\n",
    "* bag_of_words - пока остановимся на нем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "406e478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mystem = Mystem()\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    #tokens = mystem.lemmatize(text)\n",
    "    tokens = [stemmer.stem(token) for token in text.split() if token not in russian_stopwords and token != \" \"  and token.strip() not in punctuation]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def text_feature_preprocessing(text_feature):\n",
    "    '''\n",
    "    Функция преобразования текстовых факторов\n",
    "    - Переводим в нижний регистр\n",
    "    - Удаляем знаки препинания\n",
    "    - Удаляем стоп слова\n",
    "    - Проводим лемматизацию\n",
    "    '''\n",
    "    processed_feature = []\n",
    "    text_feature = text_feature.replace(r'[^\\w\\s]',' ', regex=True).replace(r'\\s+',' ', regex=True).str.lower()\n",
    "    processed_text_feature = text_feature.apply(text_preprocessing)\n",
    "    return processed_text_feature\n",
    "    #return text_feature\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "        min_df = 20,\n",
    "        analyzer='word',\n",
    "        stop_words = russian_stopwords\n",
    "    )\n",
    "\n",
    "def handle_text_feature(text_feature: pd.Series):\n",
    "    '''\n",
    "    Функция обработки строкового фактора:\n",
    "    - Проводим препроцессинг\n",
    "    - Формируем \"Мешок строк\" (bag of words)\n",
    "    '''\n",
    "    #text_feature = text_feature.drop_duplicates()\n",
    "    processed_text_feature = text_feature_preprocessing(text_feature)\n",
    "    \n",
    "    vectorizer.fit(processed_text_feature)\n",
    "    vectorized_text_feature = pd.DataFrame(vectorizer.transform(processed_text_feature).toarray())\n",
    "\n",
    "    # Удалим неинформативные столбцы\n",
    "    informative_word_columns = vectorized_text_feature.sum()[\n",
    "            (vectorized_text_feature.sum()>=vectorized_text_feature.shape[1]*0.01) &\n",
    "            (vectorized_text_feature.sum()!=vectorized_text_feature.shape[1])\n",
    "        ].index \n",
    "    handled_text_feature = vectorized_text_feature[informative_word_columns]\n",
    "    handled_text_feature = handled_text_feature.fillna('EmptyStr')\n",
    "    handled_text_feature.columns = pd.Series(vectorizer.get_feature_names_out())[informative_word_columns]\n",
    "    return handled_text_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c36d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def handle_cat_feature(cat_feature: pd.Series):\n",
    "    cat_feature = cat_feature.astype(str)\n",
    "    unique_values_count = cat_feature.drop_duplicates().size\n",
    "    if unique_values_count <= OneHotEncodingLimit:\n",
    "        #OneHotEncoding\n",
    "        cat_feature_encoded = pd.get_dummies(cat_feature)\n",
    "    else:\n",
    "        #LabelEncoding - чтобы сильно не увеличивать количество факторов\n",
    "        le = LabelEncoder()\n",
    "        cat_feature_encoded = pd.DataFrame(le.fit_transform(cat_feature))\n",
    "    return cat_feature_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f53ad5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.DataFrame()\n",
    "copy_df = not_empty_factors_df.copy()\n",
    "\n",
    "for feature in feature_types_dict:\n",
    "    if feature_types_dict.get(feature) == 'Стр':\n",
    "        handled_feature = handle_text_feature(not_empty_factors_df[feature])\n",
    "        handled_feature = handled_feature.fillna('')\n",
    "        #handled_feature = handle_text_feature(copy_df[feature])\n",
    "    elif feature_types_dict.get(feature) == 'Кат':\n",
    "        handled_feature = handle_cat_feature(not_empty_factors_df[feature])\n",
    "        #handled_feature = handle_cat_feature(copy_df[feature])\n",
    "    else:\n",
    "        handled_feature = pd.DataFrame(not_empty_factors_df[feature])\n",
    "        #handled_feature = pd.DataFrame(copy_df[feature])\n",
    "    handled_feature.columns = [str(col)+'_'+str(feature) for col in handled_feature.columns]  \n",
    "    trainset = pd.concat([trainset,handled_feature],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84bfdd78",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "chi2() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\qlever\\research\\TNVED_classification.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/qlever/research/TNVED_classification.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m chi2\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/qlever/research/TNVED_classification.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m features_chi2 \u001b[39m=\u001b[39m chi2(vectorizer\u001b[39m.\u001b[39mget_feature_names_out())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/qlever/research/TNVED_classification.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(features_chi2[\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/qlever/research/TNVED_classification.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m feature_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(vectorizer\u001b[39m.\u001b[39mget_feature_names_out())[indices]\n",
      "\u001b[1;31mTypeError\u001b[0m: chi2() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "features_chi2 = chi2(vectorizer.get_feature_names_out())\n",
    "indices = np.argsort(features_chi2[0])\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9baf2f",
   "metadata": {},
   "source": [
    "#### Сэмплируем классы, в которых всего 1 экземпляр и делим на обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db43b13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID класса (ТАРГЕТ)\n",
       "20000075    97\n",
       "20000109    91\n",
       "3584225     87\n",
       "20000112    85\n",
       "20000343    80\n",
       "            ..\n",
       "20000203     1\n",
       "20000280     1\n",
       "20000249     1\n",
       "20000582     1\n",
       "20000561     1\n",
       "Name: count, Length: 645, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abc36992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID класса (ТАРГЕТ)\n",
       "12388594    97\n",
       "20000029    97\n",
       "20000234    97\n",
       "20000214    97\n",
       "20000069    97\n",
       "            ..\n",
       "20000479    97\n",
       "20000511    97\n",
       "20000542    97\n",
       "20000587    97\n",
       "20000561    97\n",
       "Name: count, Length: 645, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "trainset_res, target_res = ros.fit_resample(trainset, target)\n",
    "target_res.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004a332",
   "metadata": {},
   "source": [
    "#### Уменьшаем размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe2c6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ce22c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 15)\n",
    "reproduced_trainset = pca.fit_transform(trainset_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f6f25998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABGcElEQVR4nO3de1xUdeL/8fdwGy7CKCg3Q8U0I+9BlrcsU0v7ddl207K0zNqszAttprWtZqVlZVbmrVarTc1ts7ZvaxmlmZcuppiVlqUmViBiBihyG87vD5wxApGDMxydeT0fDx4POXN7z8TCez+fzzkfm2EYhgAAAHxEgNUBAAAAPIlyAwAAfArlBgAA+BTKDQAA8CmUGwAA4FMoNwAAwKdQbgAAgE8JsjpAQ6uoqNAvv/yiyMhI2Ww2q+MAAIA6MAxDhYWFSkxMVEBA7WMzfldufvnlFyUlJVkdAwAA1MPevXt1xhln1Hofvys3kZGRkio/nKioKIvTAACAuigoKFBSUpL773ht/K7cuKaioqKiKDcAAJxm6rKkhAXFAADAp1BuAACAT6HcAAAAn0K5AQAAPoVyAwAAfArlBgAA+BTKDQAA8CmUGwAA4FMoNwAAwKdQbgAAgE+h3AAAAJ9CuQEAAD6FcuMhhmHowKES/ZBbaHUUAAD8GuXGQ3bnHVbqIx/oytnrZRiG1XEAAPBblBsPiXeESpKKSp0qLCm3OA0AAP6LcuMh4SFBigoNkiTtyy+2OA0AAP6LcuNBrtGbbMoNAACWodx4ULwjTJKUU0C5AQDAKpQbD4qPskuSchi5AQDAMpQbD4qPqpyWYuQGAADrUG48yDUtxYJiAACsQ7nxoHhH5bQUC4oBALAO5caD4o5OS+1jWgoAAMtQbjwo4ei01IHDpSopd1qcBgAA/0S58aAm4cEKCar8SHMLSixOAwCAf6LceJDNZuOMKQAALEa58TBXuWFRMQAA1qDceFjc0S0YOB0cAABrUG48LMHBtBQAAFai3HiY63RwtmAAAMAalBsPY0ExAADWotx4WLyDkRsAAKxEufEwV7nZV1CsigrD4jQAAPgfyo2HxUbaZbNJ5RWGDhwutToOAAB+h3LjYcGBAWraqHIDTaamAABoeJQbL2BRMQAA1qHceEE817oBAMAylBsvcI/c5B+xOAkAAP6HcuMFx04HZ2dwAAAaGuXGC1wjN/uYlgIAoMFRbrzANXKTzbQUAAANjnLjBccu5Me0FAAADY1y4wWuaalDJeUqLC6zOA0AAP6FcuMFEfYgRdqDJLHuBgCAhka58RLOmAIAwBqUGy9hUTEAANag3HgJp4MDAGANyo2XHBu5odwAANCQKDdeEsfIDQAAlqDceEkCm2cCAGAJy8vNnDlzlJycrNDQUKWmpmrt2rW13n/x4sXq3LmzwsPDlZCQoBEjRujAgQMNlLbu4tybZ1JuAABoSJaWm2XLlmncuHF64IEHlJmZqd69e2vgwIHKysqq8f7r1q3T8OHDNXLkSH3zzTd6/fXXtXHjRt16660NnPzEXCM3eYdKVVpeYXEaAAD8h6XlZubMmRo5cqRuvfVWpaSkaNasWUpKStLcuXNrvP+nn36qVq1aacyYMUpOTlavXr10++2364svvjjua5SUlKigoKDKV0OIjghRSGDlx5tbyOgNAAANxbJyU1paqk2bNmnAgAFVjg8YMEAbNmyo8TE9evTQTz/9pBUrVsgwDO3bt0//+c9/dPnllx/3daZPny6Hw+H+SkpK8uj7OB6bzabYKLskpqYAAGhIlpWbvLw8OZ1OxcXFVTkeFxennJycGh/To0cPLV68WEOGDFFISIji4+PVuHFjPffcc8d9nUmTJik/P9/9tXfvXo++j9qwqBgAgIZn+YJim81W5XvDMKodc9m2bZvGjBmjf/zjH9q0aZPee+897d69W6NGjTru89vtdkVFRVX5aigsKgYAoOEFWfXCTZs2VWBgYLVRmtzc3GqjOS7Tp09Xz549de+990qSOnXqpIiICPXu3VuPPPKIEhISvJ7bjHjKDQAADc6ykZuQkBClpqYqIyOjyvGMjAz16NGjxscUFRUpIKBq5MDAQEmVIz6nmnimpQAAaHCWTkulp6frxRdf1MKFC7V9+3aNHz9eWVlZ7mmmSZMmafjw4e77X3HFFVq+fLnmzp2rXbt2af369RozZoy6deumxMREq97GcR3bGZxyAwBAQ7FsWkqShgwZogMHDmjq1KnKzs5Whw4dtGLFCrVs2VKSlJ2dXeWaNzfffLMKCws1e/Zs3XPPPWrcuLH69u2rxx9/3Kq3UCsWFAMA0PBsxqk4n+NFBQUFcjgcys/P9/ri4p8OFqnX46sVEhigbx++TAEBNS+UBgAAtTPz99vys6V8WWxk5chNqbNCvxaVWpwGAAD/QLnxopCgADVtxIX8AABoSJQbL4t3UG4AAGhIlBsvi48Kk8SiYgAAGgrlxstcIzf7KDcAADQIyo2Xua5SnM20FAAADYJy42XxjsppKUZuAABoGJQbL2PkBgCAhkW58TL3mhvKDQAADYJy42WuaanCknIdKim3OA0AAL6PcuNljexBamSv3MKLa90AAOB9lJsG4NodnEXFAAB4H+WmAbgWFTNyAwCA91FuGkCcq9wwcgMAgNdRbhpAgoORGwAAGgrlpgHEObjWDQAADaXe5aa0tFTfffedyss5vflEXGtuWFAMAID3mS43RUVFGjlypMLDw9W+fXtlZWVJksaMGaPHHnvM4wF9gXtainIDAIDXmS43kyZN0pdffqmPPvpIoaGh7uP9+vXTsmXLPBrOV7gWFOcdKlGZs8LiNAAA+LYgsw946623tGzZMl1wwQWy2Wzu4+ecc4527tzp0XC+IiYiRMGBNpU5DeUWlqh54zCrIwEA4LNMj9zs379fsbGx1Y4fPny4StnBMQEBNsVGcsYUAAANwXS5Oe+88/S///3P/b2r0Lzwwgvq3r2755L5mHhOBwcAoEGYnpaaPn26LrvsMm3btk3l5eV65pln9M033+iTTz7RmjVrvJHRJ8SzqBgAgAZheuSmR48eWr9+vYqKinTmmWfq/fffV1xcnD755BOlpqZ6I6NPOLYFwxGLkwAA4NtMj9xIUseOHfXyyy97OotPO3Y6eInFSQAA8G2mR25WrFihlStXVju+cuVKvfvuux4J5Ytcp4PvY80NAABeZbrcTJw4UU6ns9pxwzA0ceJEj4TyRa41N9kFTEsBAOBNpsvN999/r3POOafa8bPPPls//PCDR0L5IvcWDPklMgzD4jQAAPgu0+XG4XBo165d1Y7/8MMPioiI8EgoX+Salip1VujXw6UWpwEAwHeZLjdXXnmlxo0bV+VqxD/88IPuueceXXnllR4N50tCggIUExEiidPBAQDwJtPl5oknnlBERITOPvtsJScnKzk5WSkpKYqJidGTTz7pjYw+w7Xuht3BAQDwHtOngjscDm3YsEEZGRn68ssvFRYWpk6dOunCCy/0Rj6fEh8Vqm9+KVA2Z0wBAOA19brOjc1m04ABAzRgwABP5/Fp7pEbyg0AAF5Tr3Lz4Ycf6sMPP1Rubq4qKiqq3LZw4UKPBPNF7qsUMy0FAIDXmC43Dz30kKZOnaq0tDQlJCSwE7gJca5r3TByAwCA15guN/PmzdNLL72kYcOGeSOPT0tgQTEAAF5n+myp0tJS9ejRwxtZfJ5rWoqRGwAAvMd0ubn11lu1ZMkSb2Txea4FxYXF5SoqLbc4DQAAvsn0tFRxcbEWLFigDz74QJ06dVJwcHCV22fOnOmxcL4mMjRYESGBOlzqVE5+sVo3a2R1JAAAfI7pcrN161Z16dJFkvT1119XuY3FxScW5wjVrv2HKTcAAHiJ6XKzevVqb+TwGwmucsOiYgAAvML0mhucnDiudQMAgFfV6yJ+Gzdu1Ouvv66srCyVllbd4Xr58uUeCear3Bfy44wpAAC8wvTIzWuvvaaePXtq27ZtevPNN1VWVqZt27Zp1apVcjgc3sjoU1zXuqHcAADgHabLzbRp0/T000/rnXfeUUhIiJ555hlt375dgwcPVosWLbyR0acwLQUAgHeZLjc7d+7U5ZdfLkmy2+06fPiwbDabxo8frwULFng8oK9JcIRJYuQGAABvMV1uoqOjVVhYKElq3ry5+3Tw3377TUVFRZ5N54PiHHZJ0v5DJSpzVpzg3gAAwCzT5aZ3797KyMiQJA0ePFhjx47Vbbfdpuuvv16XXHKJxwP6mqYRdgUF2GQY0v7CEqvjAADgc0yfLTV79mwVF1dOqUyaNEnBwcFat26drrnmGj344IMeD+hrAgJsiosK1c+/HVFOQbESG4dZHQkAAJ9iutxER0e7/x0QEKAJEyZowoQJHg3l6+Ki7JXlhnU3AAB4XJ3KTUFBgaKiotz/ro3rfji+ykXFv1FuAADwgjqVmyZNmig7O1uxsbFq3LhxjXtIGYYhm80mp9Pp8ZC+xnU6+D5OBwcAwOPqVG5WrVrlno5ib6mTF3/0jKlsRm4AAPC4OpWbPn36SJLKy8v10Ucf6ZZbblFSUpJXg/myeNe1bhi5AQDA40ydCh4UFKQnn3ySqaeTFM+0FAAAXmP6OjeXXHKJPvroIy9E8R+ucpOdXyzDMCxOAwCAbzF9KvjAgQM1adIkff3110pNTVVERESV26+88kqPhfNVsVGVa25Kyyv0W1GZmkSEWJwIAADfYbrc3HHHHZKkmTNnVruNs6XqJjQ4UNERIfr1cKmy84spNwAAeJDpaamKiorjflFs6o51NwAAeIfpcgPPiHdUlhvOmAIAwLNMT0tJ0uHDh7VmzRplZWWptLS0ym1jxozxSDBfF/e7RcUAAMBzTJebzMxMDRo0SEVFRTp8+LCio6OVl5en8PBwxcbGUm7qKOHoyM0+yg0AAB5lelpq/PjxuuKKK/Trr78qLCxMn376qfbs2aPU1FQ9+eST3sjok1xrbpiWAgDAs0yXmy1btuiee+5RYGCgAgMDVVJSoqSkJM2YMUP333+/6QBz5sxRcnKyQkNDlZqaqrVr19Z6/5KSEj3wwANq2bKl7Ha7zjzzTC1cuND061otzrXmhpEbAAA8yvS0VHBwsHvjzLi4OGVlZSklJUUOh0NZWVmmnmvZsmUaN26c5syZo549e2r+/PkaOHCgtm3bphYtWtT4mMGDB2vfvn365z//qTZt2ig3N1fl5eVm34blElhQDACAV5guN127dtUXX3yhs846SxdffLH+8Y9/KC8vT//617/UsWNHU881c+ZMjRw5UrfeeqskadasWVq5cqXmzp2r6dOnV7v/e++9pzVr1mjXrl3ujTxbtWpV62uUlJSopKTE/X1BQYGpjN7iWlCcf6RMR0qdCgsJtDgRAAC+wfS01LRp05SQkCBJevjhhxUTE6M77rhDubm5WrBgQZ2fp7S0VJs2bdKAAQOqHB8wYIA2bNhQ42PefvttpaWlacaMGWrevLnOOuss/e1vf9ORI0eO+zrTp0+Xw+Fwf50qG35GhQYp/GihYfQGAADPMT1yk5aW5v53s2bNtGLFinq9cF5enpxOp+Li4qocj4uLU05OTo2P2bVrl9atW6fQ0FC9+eabysvL05133qlff/31uOtuJk2apPT0dPf3BQUFp0TBsdlsio8K1a68w8rJL1Zy04gTPwgAAJyQ6ZGbhx56SDt37vRYANf6HRfDMKodc6moqJDNZtPixYvVrVs3DRo0SDNnztRLL7103NEbu92uqKioKl+nijj3GVPHH3kCAADmmC43b7zxhs466yxdcMEFmj17tvbv31+vF27atKkCAwOrjdLk5uZWG81xSUhIUPPmzeVwONzHUlJSZBiGfvrpp3rlsJJ7UXF+yQnuCQAA6sp0udm6dau2bt2qvn37aubMmWrevLkGDRqkJUuWqKioqM7PExISotTUVGVkZFQ5npGRoR49etT4mJ49e+qXX37RoUOH3Md27NihgIAAnXHGGWbfiuWOnQ7OyA0AAJ5Sr72l2rdvr2nTpmnXrl1avXq1kpOTNW7cOMXHx5t6nvT0dL344otauHChtm/frvHjxysrK0ujRo2SVLleZvjw4e77Dx06VDExMRoxYoS2bdumjz/+WPfee69uueUWhYWF1eetWIrTwQEA8Lx67S31exEREQoLC1NISIgKCwtNPXbIkCE6cOCApk6dquzsbHXo0EErVqxQy5YtJUnZ2dlVrp3TqFEjZWRk6O6771ZaWppiYmI0ePBgPfLIIyf7NixxbM0N01IAAHiKzTAMw+yDdu/erSVLlmjx4sXasWOHLrzwQg0dOlTXXnttlfUwp6KCggI5HA7l5+dbvrj4y72/6arn1ysuyq7P7u9naRYAAE5lZv5+mx656d69uz7//HN17NhRI0aM0NChQ9W8efN6h/Vnrmmp/YUlKndWKCiwXrOEAADgd0yXm4svvlgvvvii2rdv7408fiWmkV2BATY5KwzlHSpV/NGyAwAA6s90uZk2bZo3cvilwACbYiPtys4vVnb+EcoNAAAewDyIxVyFZh9nTAEA4BGUG4vFHz1jKjufcgMAgCdQbiwWz7VuAADwKMqNxVwjN/sYuQEAwCPqtKB469atdX7CTp061TuMP3KN3DAtBQCAZ9Sp3HTp0kU2m63WHbtdnE6nR4L5C/fIDdNSAAB4RJ2mpXbv3q1du3Zp9+7deuONN5ScnKw5c+YoMzNTmZmZmjNnjs4880y98cYb3s7rc36/5qYeF4sGAAB/UKeRG9deT5J07bXX6tlnn9WgQYPcxzp16qSkpCQ9+OCDuvrqqz0e0pe59pcqLqtQ/pEyNQ4PsTgRAACnN9MLir/66islJydXO56cnKxt27Z5JJQ/CQ0OVJPwYEmcMQUAgCeYLjcpKSl65JFHVFx87A9xSUmJHnnkEaWkpHg0nL+I41o3AAB4jOntF+bNm6crrrhCSUlJ6ty5syTpyy+/lM1m0zvvvOPxgP4gwRGqb3MKOR0cAAAPMF1uunXrpt27d+vVV1/Vt99+K8MwNGTIEA0dOlQRERHeyOjzuJAfAACeY7rcSFJ4eLj++te/ejqL33JNS+UwcgMAwEmr1xWK//Wvf6lXr15KTEzUnj17JElPP/20/vvf/3o0nL9IYOQGAACPMV1u5s6dq/T0dA0cOFAHDx50X7SvSZMmmjVrlqfz+QVGbgAA8BzT5ea5557TCy+8oAceeEBBQcdmtdLS0vTVV195NJy/SHCESWLkBgAATzBdbnbv3q2uXbtWO26323X48GGPhPI3ri0YfisqU3EZ21cAAHAyTJeb5ORkbdmypdrxd999V+ecc44nMvmdqLAghQZX/qdgagoAgJNj+mype++9V3fddZeKiyv3Qvr888+1dOlSTZ8+XS+++KI3Mvo8m82mBEeYducdVk5BsVo15ZR6AADqy3S5GTFihMrLyzVhwgQVFRVp6NChat68uZ555hldd9113sjoF+Ki7Nqdd5jdwQEAOEn1us7Nbbfdpttuu015eXmqqKhQbGysp3P5HdeiYrZgAADg5NSr3Lg0bdrUUzn8HqeDAwDgGaYXFO/bt0/Dhg1TYmKigoKCFBgYWOUL9RMfZZdEuQEA4GSZHrm5+eablZWVpQcffFAJCQmy2WzeyOV34rnWDQAAHmG63Kxbt05r165Vly5dvBDHf7k2z2RBMQAAJ8f0tFRSUpIMw/BGFr/mupBfbmGJnBV8vgAA1JfpcjNr1ixNnDhRP/74oxfi+K9mkXYFBtjkrDCUd6jE6jgAAJy2TE9LDRkyREVFRTrzzDMVHh6u4ODgKrf/+uuvHgvnTwIDbGrWyK6cgmLl5Be7z54CAADmmC437PztPfGOUOUUFCs7v1idk6xOAwDA6cl0ubnpppu8kQM6tu6GRcUAANRfncpNQUGBoqKi3P+ujet+MM91xhRXKQYAoP7qVG6aNGmi7OxsxcbGqnHjxjVe28YwDNlsNjmdTo+H9BecDg4AwMmrU7lZtWqVoqOjJUmrV6/2aiB/Fs8WDAAAnLQ6lZs+ffrU+G94lnt/KUZuAACot3pvnFlUVKSsrCyVlpZWOd6pU6eTDuWvEhzHRm5c03wAAMAc0+Vm//79GjFihN59990ab2fNTf251twcKXOq4Ei5HOHBJ3gEAAD4I9NXKB43bpwOHjyoTz/9VGFhYXrvvff08ssvq23btnr77be9kdFvhAYHqvHRQsPUFAAA9WN65GbVqlX673//q/POO08BAQFq2bKl+vfvr6ioKE2fPl2XX365N3L6jfioUP1WVKacgmK1i4+0Og4AAKcd0yM3hw8fVmxsrCQpOjpa+/fvlyR17NhRmzdv9mw6P+ReVJx/xOIkAACcnkyXm3bt2um7776TJHXp0kXz58/Xzz//rHnz5ikhIcHjAf3NsUXFbJ4JAEB9mJ6WGjdunLKzsyVJkydP1qWXXqrFixcrJCREL730kqfz+R1OBwcA4OSYLjc33HCD+99du3bVjz/+qG+//VYtWrRQ06ZNPRrOHx0buWFaCgCA+qj3dW5cwsPDde6553oiCyTFucpNAdNSAADUR53KTXp6ep2fcObMmfUOg99vwcDIDQAA9VGncpOZmVmnJ+OKuifPNS11sKhMxWVOhQYHWpwIAIDTS53KDZtlNhxHWLDsQQEqKa9QbkGJWsSEWx0JAIDTiulTwX9v7969+umnnzyVBaoc/XJtw5DN1BQAAKaZLjfl5eV68MEH5XA41KpVK7Vs2VIOh0N///vfVVZW5o2Mfiee08EBAKg302dLjR49Wm+++aZmzJih7t27S5I++eQTTZkyRXl5eZo3b57HQ/ob18jNPsoNAACmmS43S5cu1WuvvaaBAwe6j3Xq1EktWrTQddddR7nxgGPTUpQbAADMMj0tFRoaqlatWlU73qpVK4WEhHgik99zTUsxcgMAgHmmy81dd92lhx9+WCUlxy4yV1JSokcffVSjR4/2aDh/5So3jNwAAGCe6WmpzMxMffjhhzrjjDPUuXNnSdKXX36p0tJSXXLJJbrmmmvc912+fLnnkvoR95obyg0AAKaZLjeNGzfWn//85yrHkpKSPBYIx8pNbmGJnBWGAgO4OCIAAHVlutwsWrTIGznwO80a2RVgk8orDB04VKLYo9NUAADgxEyvufnmm2+Oe9t77713UmFQKSgwQM0i7ZK41g0AAGaZLjdpaWl67rnnqhwrKSnR6NGj9ac//cljwfwdi4oBAKgf0+Vm8eLFeuihhzRw4EDl5ORoy5Yt6tq1q1atWqX169d7I6Nf4kJ+AADUj+lyc80112jr1q0qLy9Xhw4d1L17d1100UXatGmTzj33XG9k9EvuLRgYuQEAwJR6bZzpdDpVWloqp9Mpp9Op+Ph42e12T2fza3EOyg0AAPVhuty89tpr6tSpkxwOh3bs2KH//e9/WrBggXr37q1du3aZDjBnzhwlJycrNDRUqampWrt2bZ0et379egUFBalLly6mX/N0kOBg80wAAOrDdLkZOXKkpk2bprffflvNmjVT//799dVXX6l58+ami8ayZcs0btw4PfDAA8rMzFTv3r01cOBAZWVl1fq4/Px8DR8+XJdcconZ+KeNOHYGBwCgXkyXm82bN+uOO+6ocqxJkyb697//reeff97Uc82cOVMjR47UrbfeqpSUFM2aNUtJSUmaO3durY+7/fbbNXToUPeu5L4owREmqXJayjAMi9MAAHD6MF1u2rVrp/Lycn3wwQeaP3++CgsLJUm//PKLqVPBS0tLtWnTJg0YMKDK8QEDBmjDhg3HfdyiRYu0c+dOTZ48uU6vU1JSooKCgipfpwPXguKiUqcKS8otTgMAwOnD9BWK9+zZo8suu0xZWVkqKSlR//79FRkZqRkzZqi4uFjz5s2r0/Pk5eXJ6XQqLi6uyvG4uDjl5OTU+Jjvv/9eEydO1Nq1axUUVLfo06dP10MPPVSn+55KwkICFRUapILicuXkFysqNNjqSAAAnBZMj9yMHTtWaWlpOnjwoMLCwtzH//SnP+nDDz80HcBmq7pvkmEY1Y5JlWdoDR06VA899JDOOuusOj//pEmTlJ+f7/7au3ev6YxW+f3UFAAAqBvTIzfr1q3T+vXrFRISUuV4y5Yt9fPPP9f5eZo2barAwMBqozS5ubnVRnMkqbCwUF988YUyMzM1evRoSVJFRYUMw1BQUJDef/999e3bt9rj7Hb7aXuaepwjVN/tK2RRMQAAJpgeuamoqJDT6ax2/KefflJkZGSdnyckJESpqanKyMiocjwjI0M9evSodv+oqCh99dVX2rJli/tr1KhRateunbZs2aLzzz/f7Fs55cVHHd1fipEbAADqzPTITf/+/TVr1iwtWLBAUuW00qFDhzR58mQNGjTI1HOlp6dr2LBhSktLU/fu3bVgwQJlZWVp1KhRkiqnlH7++We98sorCggIUIcOHao8PjY2VqGhodWO+4p417QUIzcAANSZ6XLz9NNP6+KLL9Y555yj4uJiDR06VN9//72aNm2qpUuXmnquIUOG6MCBA5o6daqys7PVoUMHrVixQi1btpQkZWdnn/CaN77MdcbUPkZuAACoM5tRj4uoHDlyRK+99po2bdqkiooKnXvuubrhhhuqLDA+VRUUFMjhcCg/P19RUVFWx6nV6m9zNeKljTonIUorxva2Og4AAJYx8/fb9MiNJIWFhWnEiBEaMWJEvQKiblxXKWZncAAA6q5eG2eiYcQf3V/qwOFSlZRXX8QNAACqo9ycwpqEByskqPI/UW5BicVpAAA4PVBuTmE2m829qJgzpgAAqBvKzSnONTWVzRlTAADUSb3KzW+//aYXX3xRkyZN0q+//iqpcrdwM1coRt1wOjgAAOaYPltq69at6tevnxwOh3788Ufddtttio6O1ptvvqk9e/bolVde8UZOv8XIDQAA5pgeuUlPT9fNN9+s77//XqGhoe7jAwcO1Mcff+zRcPjdyA1rbgAAqBPT5Wbjxo26/fbbqx1v3rx5tU0wcfJcIzcsKAYAoG5Ml5vQ0FAVFBRUO/7dd9+pWbNmHgmFY1wX8mPzTAAA6sZ0ubnqqqs0depUlZWVSao8XTkrK0sTJ07Un//8Z48H9HcJjmPTUhUVpnfKAADA75guN08++aT279+v2NhYHTlyRH369FGbNm0UGRmpRx991BsZ/VqzSLtsNqm8wtCBw6VWxwEA4JRn+mypqKgorVu3TqtWrdLmzZvdG2f269fPG/n8XnBggJo1siu3sEQ5+cVqFmm3OhIAAKc00+Xmxx9/VKtWrdS3b1/17dvXG5nwB/GO0MpyU1CsjnJYHQcAgFOa6Wmp1q1bq1evXpo/f777An7wrmOLio9YnAQAgFOf6XLzxRdfqHv37nrkkUeUmJioq666Sq+//rpKStjY0VsSOB0cAIA6M11uzj33XD3xxBPKysrSu+++q9jYWN1+++2KjY3VLbfc4o2Mfu/YyA0FEgCAE6n3xpk2m00XX3yxXnjhBX3wwQdq3bq1Xn75ZU9mw1HHRm6YlgIA4ETqXW727t2rGTNmqEuXLjrvvPMUERGh2bNnezIbjornQn4AANSZ6bOlFixYoMWLF2v9+vVq166dbrjhBr311ltq1aqVF+JBkuLcF/JjWgoAgBMxXW4efvhhXXfddXrmmWfUpUsXL0TCH7lGbg6VlKuwuEyRocEWJwIA4NRlutxkZWXJZrN5IwuOI8IepMjQIBUWl2tfQTHlBgCAWtSp3GzdulUdOnRQQECAvvrqq1rv26lTJ48EQ1XxUaEqLD6k7PxitYmNtDoOAACnrDqVmy5duignJ0exsbHq0qWLbDabDOPYJo6u7202m5xOp9fC+rN4R6i+zz3EomIAAE6gTuVm9+7datasmfvfaHiudTf7uJAfAAC1qlO5admypfvfe/bsUY8ePRQUVPWh5eXl2rBhQ5X7wnNc17rJZuQGAIBamb7OzcUXX1zjnlL5+fm6+OKLPRIK1R07HZxyAwBAbUyXG9famj86cOCAIiIiPBIK1bmmpRi5AQCgdnU+Ffyaa66RVLl4+Oabb5bdbnff5nQ6tXXrVvXo0cPzCSGpckGxxMgNAAAnUudy43A4JFWO3ERGRiosLMx9W0hIiC644ALddtttnk8IScdGbvIOlaq0vEIhQfXeOQMAAJ9W53KzaNEiSVKrVq30t7/9jSmoBhYdEaKQwACVOiu0r6BYSdHhVkcCAOCUZPr//k+ePJliYwGbzaY4R+VUIFNTAAAcn+ntFyTpP//5j/79738rKytLpaWlVW7bvHmzR4KhuvioUO399YhyKDcAAByX6ZGbZ599ViNGjFBsbKwyMzPVrVs3xcTEaNeuXRo4cKA3MuKoeEflOieuUgwAwPGZLjdz5szRggULNHv2bIWEhGjChAnKyMjQmDFjlJ+f742MOCo+qnJainIDAMDxmS43WVlZ7lO+w8LCVFhYKEkaNmyYli5d6tl0qCLOda0bpqUAADgu0+UmPj5eBw4ckFS5LcOnn34qqXLPqd9vpgnPSzg6LbWPkRsAAI7LdLnp27ev/u///k+SNHLkSI0fP179+/fXkCFD9Kc//cnjAXFM/NGzpVhQDADA8Zk+W2rBggWqqKiQJI0aNUrR0dFat26drrjiCo0aNcrjAXGMa0HxvoJiVVQYCgiovg0GAAD+znS5CQgIUEDAsQGfwYMHa/DgwR4NhZrFRtpls0llTkO/FpWqaSP7iR8EAICfqVO52bp1a52fsFOnTvUOg9oFBwYoJsKuvEMlyskvptwAAFCDOpWbLl26yGaznXDBsM1mk9Pp9Egw1CzBEeouNx2aO6yOAwDAKadO5Wb37t3ezoE6iosK1Vc/57OoGACA46hTuWnZsqW3c6CO3GdMcTo4AAA1Mr2g+JVXXqn19uHDh9c7DE7Mda0bRm4AAKiZ6XIzduzYKt+XlZWpqKhIISEhCg8Pp9x4mesqxewMDgBAzUxfxO/gwYNVvg4dOqTvvvtOvXr1YvuFBpDgOLoFA9NSAADUyHS5qUnbtm312GOPVRvVgee5R24oNwAA1Mgj5UaSAgMD9csvv3jq6XAc8UdHbgpLynWopNziNAAAnHpMr7l5++23q3xvGIays7M1e/Zs9ezZ02PBULNG9iBF2oNUWFKunPxitYltZHUkAABOKabLzdVXX13le5vNpmbNmqlv37566qmnPJULtYhzhKow95D2FVBuAAD4I9PlxrVpJqyT4AjVD7mHWFQMAEANPLbmBg2H08EBADg+0yM3hmHoP//5j1avXq3c3NxqIznLly/3WDjULP5oueEqxQAAVFevi/gtWLBAF198seLi4mSz2byRC7WI51o3AAAcl+ly8+qrr2r58uUaNGiQN/KgDuKZlgIA4LhMr7lxOBxq3bq1N7Kgjhi5AQDg+EyXmylTpuihhx7SkSNHvJEHdeAqNwcOl6jMydlrAAD8nulpqWuvvVZLly5VbGysWrVqpeDg4Cq3b9682WPhULPo8BAFB9pU5jSUW1ii5o3DrI4EAMApw3S5ufnmm7Vp0ybdeOONLCi2SECATXFRofrp4BHl5B+h3AAA8Dumy83//vc/rVy5Ur169fJGHtRRvLvclFgdBQCAU4rpNTdJSUmKioryRhaYEHd03U0OZ0wBAFCF6XLz1FNPacKECfrxxx+9EAd1leC+kB8LuwEA+D3T5ebGG2/U6tWrdeaZZyoyMlLR0dFVvsyaM2eOkpOTFRoaqtTUVK1du/a4912+fLn69++vZs2aKSoqSt27d9fKlStNv6YviHeP3DAtBQDA75leczNr1iyPvfiyZcs0btw4zZkzRz179tT8+fM1cOBAbdu2TS1atKh2/48//lj9+/fXtGnT1LhxYy1atEhXXHGFPvvsM3Xt2tVjuU4H7nLDyA0AAFXYDMMwrHrx888/X+eee67mzp3rPpaSkqKrr75a06dPr9NztG/fXkOGDNE//vGPOt2/oKBADodD+fn5p/XaoS9+/FV/mfeJkqLDtHZCX6vjAADgVWb+fpseucnKyqr19ppGXGpSWlqqTZs2aeLEiVWODxgwQBs2bKjTc1RUVKiwsLDW6bCSkhKVlBybuikoKKjTc5/q3DuD55fIMAxOyQcA4CjT5aZVq1a1/iF1Op11ep68vDw5nU7FxcVVOR4XF6ecnJw6PcdTTz2lw4cPa/Dgwce9z/Tp0/XQQw/V6flOJ65yU+qs0K+HSxXTyG5xIgAATg2my01mZmaV78vKypSZmamZM2fq0UcfNR3gj0WprqMQS5cu1ZQpU/Tf//5XsbGxx73fpEmTlJ6e7v6+oKBASUlJpnOeakKCAtS0UYjyDpUqp6CYcgMAwFGmy03nzp2rHUtLS1NiYqKeeOIJXXPNNXV6nqZNmyowMLDaKE1ubm610Zw/WrZsmUaOHKnXX39d/fr1q/W+drtddrtv/uGPiwqtLDf5xWqf6LA6DgAApwTTp4Ifz1lnnaWNGzfW+f4hISFKTU1VRkZGleMZGRnq0aPHcR+3dOlS3XzzzVqyZIkuv/zyeuf1BQlcyA8AgGpMj9z8cUGuYRjKzs7WlClT1LZtW1PPlZ6ermHDhiktLU3du3fXggULlJWVpVGjRkmqnFL6+eef9corr0iqLDbDhw/XM888owsuuMA96hMWFiaHw/9GLo4tKqbcAADgYrrcNG7cuMZ1MklJSXrttddMPdeQIUN04MABTZ06VdnZ2erQoYNWrFihli1bSpKys7OrnJ01f/58lZeX66677tJdd93lPn7TTTfppZdeMvtWTnuukZtsyg0AAG6mr3Pz0UcfVSk3AQEBatasmdq0aaOgINNdqcH5ynVuJOn1L/bq3v9sVe+2TfWvkedbHQcAAK/x6nVuLrroovrmgoe5rlK8jzU3AAC4mV5QPH36dC1cuLDa8YULF+rxxx/3SCjUDdNSAABUZ7rczJ8/X2effXa14+3bt9e8efM8Egp141pQXFhcrsMl5RanAQDg1GC63OTk5CghIaHa8WbNmik7O9sjoVA3kaHBamSvnFnkdHAAACqZLjdJSUlav359tePr169XYmKiR0Kh7uKiKi9QyOngAABUMr2g+NZbb9W4ceNUVlamvn0rd6P+8MMPNWHCBN1zzz0eD4jaxTtCtXP/YUZuAAA4ynS5mTBhgn799VfdeeedKi0tlSSFhobqvvvu06RJkzweELWLjwqTxKJiAABcTJcbm82mxx9/XA8++KC2b9+usLAwtW3b1mf3bzrVxTuOTksxcgMAgKR6lBuXRo0a6bzzzvNkFtRD/NEzpnIYuQEAQJIHN86ENeIdldNSrLkBAKAS5eY0x8gNAABVUW5Oc64tGPYfKlGZs8LiNAAAWI9yc5qLiQhRcKBNhiHtLyyxOg4AAJaj3JzmAgJsio08OjXFuhsAACg3vsA1NcW6GwAAKDc+gUXFAAAcQ7nxAa7dwZmWAgCAcuMTEpiWAgDAjXLjA+IcjNwAAOBCufEBjNwAAHAM5cYHxP9uzY1hGBanAQDAWpQbHxAbVbkzeGl5hX4rKrM4DQAA1qLc+AB7UKBiIkIkSdlMTQEA/Bzlxke4Tgffx6JiAICfo9z4CNeiYkZuAAD+jnLjIzgdHACASpQbH+E6Y2ofIzcAAD9HufERrs0zsxm5AQD4OcqNj2DkBgCASpQbHxHPmhsAACRRbnyGq9zkHynTkVKnxWkAALAO5cZHRNqDFB4SKEn6NqfA4jQAAFiHcuMjbDabzk+OliTdtXizfvntiMWJAACwBuXGhzx5bWe1bhahX/KLNeyfn+nXw6VWRwIAoMFRbnxITCO7/jXyfCU4QrVz/2HdvOhzHSoptzoWAAANinLjY5o3DtO/Rp6vJuHB2vpTvv76yhcqLmOBMQDAf1BufFCb2EZ6+ZZuiggJ1IadBzT2tUyVOyusjgUAQIOg3PioTmc01gvD0xQSGKCV3+zTA29+LcMwrI4FAIDXUW58WI82TfXs9V0VYJOWfbFXj733rdWRAADwOsqNj7usQ7weu6aTJGn+ml2at2anxYkAAPAuyo0fGHxekiYNPFuS9Ni73+q1z7MsTgQAgPdQbvzE7X3O1Kg+Z0qS7n/zK733dbbFiQAA8A7KjR+577J2uu68JFUY0pilW7T+hzyrIwEA4HGUGz9is9n06J866rL28Sp1Vuivr3yhL/f+ZnUsAAA8inLjZwIDbHrm+i7q2SZGh0udunnR5/oht9DqWAAAeAzlxg/ZgwI1f1iaOp/h0MGiMg375+f6mY02AQA+gnLjpxrZg7RoRDe1iW2k7PxiDXvxMx04VGJ1LAAAThrlxo9FR4ToXyO7qXnjMO3KO6ybFn2uwuIyq2MBAHBSKDd+LsERpldGdlNMRIi+/rlAt7HRJgDgNEe5gc5s1kgvjeimRvYgfbrrV929lI02AQCnL8oNJEkdz3BUbrQZFKCMbfs0cflXbLQJADgtUW7g1v3MGM0+utHmfzb9pGkrtlNwAACnHcoNqhjQPl6P/7lyo80X1u7WXDbaBACcZig3qObatCT9/fIUSdKM977Tks/YaBMAcPqg3KBGt/ZurTsvqtxo84G3vtKKr9hoEwBweqDc4LjuvbSdru/WQoYhjX0tU2u/3291JAAATohyg+Oy2Wx65OoOurxjgsqchm7/1yZlZh20OhYAALWi3KBWgQE2zRzSWb3bNlVRqVMjXtqoHfvYaBMAcOqi3OCE7EGBmndjqrokNdZvRWUa9s/PtPfXIqtjAQBQI8oN6iTCHqRFN5+ntrGNtK+gRMMXfq79hWy0CQA49VBuUGdNIkL0r5Hnq3njMO3OO6ybF32uAjbaBACcYig3MCXeEapXbz1fMREh+uaXAt36MhttAgBOLZQbmJbcNEIv39JNkfYgfb77V41espmNNgEApwzLy82cOXOUnJys0NBQpaamau3atbXef82aNUpNTVVoaKhat26tefPmNVBS/F6H5g69eFOa7EEB+mB7ria8sVUVFexDBQCwnqXlZtmyZRo3bpweeOABZWZmqnfv3ho4cKCysmq+3P/u3bs1aNAg9e7dW5mZmbr//vs1ZswYvfHGGw2cHJJ0fusYPT/0XAUG2LR888965H9stAkAsJ7NsPCv0fnnn69zzz1Xc+fOdR9LSUnR1VdfrenTp1e7/3333ae3335b27dvdx8bNWqUvvzyS33yySd1es2CggI5HA7l5+crKirq5N8E9Mamn3TP619Kksb1a6trup5R58fabJ7P443nBADUXWCATQmOMI8+p5m/30EefWUTSktLtWnTJk2cOLHK8QEDBmjDhg01PuaTTz7RgAEDqhy79NJL9c9//lNlZWUKDg6u9piSkhKVlBw7ZbmgoMAD6fF7f049Q78dKdPD72zTrA++16wPvrc6EgDAQrGRdn3+QD/LXt+ycpOXlyen06m4uLgqx+Pi4pSTk1PjY3Jycmq8f3l5ufLy8pSQkFDtMdOnT9dDDz3kueCo0cheySpzVmjemp0qLa//4uKTHUc0xLQYYDVmp2EPtnZJr2XlxsX2hzkEwzCqHTvR/Ws67jJp0iSlp6e7vy8oKFBSUlJ946IWo/qcqVF9zrQ6BgDAz1lWbpo2barAwMBqozS5ubnVRmdc4uPja7x/UFCQYmJianyM3W6X3W73TGgAAHDKs2zcKCQkRKmpqcrIyKhyPCMjQz169KjxMd27d692//fff19paWk1rrcBAAD+x9JJsfT0dL344otauHChtm/frvHjxysrK0ujRo2SVDmlNHz4cPf9R40apT179ig9PV3bt2/XwoUL9c9//lN/+9vfrHoLAADgFGPpmpshQ4bowIEDmjp1qrKzs9WhQwetWLFCLVu2lCRlZ2dXueZNcnKyVqxYofHjx+v5559XYmKinn32Wf35z3+26i0AAIBTjKXXubEC17kBAOD0Y+bvt+XbLwAAAHgS5QYAAPgUyg0AAPAplBsAAOBTKDcAAMCnUG4AAIBPodwAAACfQrkBAAA+hXIDAAB8iqXbL1jBdUHmgoICi5MAAIC6cv3drsvGCn5XbgoLCyVJSUlJFicBAABmFRYWyuFw1Hofv9tbqqKiQr/88osiIyNls9k8+twFBQVKSkrS3r17/XLfKn9//xKfgb+/f4nPgPfv3+9f8t5nYBiGCgsLlZiYqICA2lfV+N3ITUBAgM444wyvvkZUVJTf/lBLvH+Jz8Df37/EZ8D79+/3L3nnMzjRiI0LC4oBAIBPodwAAACfQrnxILvdrsmTJ8tut1sdxRL+/v4lPgN/f/8SnwHv37/fv3RqfAZ+t6AYAAD4NkZuAACAT6HcAAAAn0K5AQAAPoVyAwAAfArlxkPmzJmj5ORkhYaGKjU1VWvXrrU6UoOZPn26zjvvPEVGRio2NlZXX321vvvuO6tjWWb69Omy2WwaN26c1VEa1M8//6wbb7xRMTExCg8PV5cuXbRp0yarYzWI8vJy/f3vf1dycrLCwsLUunVrTZ06VRUVFVZH85qPP/5YV1xxhRITE2Wz2fTWW29Vud0wDE2ZMkWJiYkKCwvTRRddpG+++caasF5Q2/svKyvTfffdp44dOyoiIkKJiYkaPny4fvnlF+sCe8GJfgZ+7/bbb5fNZtOsWbMaJBvlxgOWLVumcePG6YEHHlBmZqZ69+6tgQMHKisry+poDWLNmjW666679OmnnyojI0Pl5eUaMGCADh8+bHW0Brdx40YtWLBAnTp1sjpKgzp48KB69uyp4OBgvfvuu9q2bZueeuopNW7c2OpoDeLxxx/XvHnzNHv2bG3fvl0zZszQE088oeeee87qaF5z+PBhde7cWbNnz67x9hkzZmjmzJmaPXu2Nm7cqPj4ePXv39+9v9/prrb3X1RUpM2bN+vBBx/U5s2btXz5cu3YsUNXXnmlBUm950Q/Ay5vvfWWPvvsMyUmJjZQMkkGTlq3bt2MUaNGVTl29tlnGxMnTrQokbVyc3MNScaaNWusjtKgCgsLjbZt2xoZGRlGnz59jLFjx1odqcHcd999Rq9evayOYZnLL7/cuOWWW6ocu+aaa4wbb7zRokQNS5Lx5ptvur+vqKgw4uPjjccee8x9rLi42HA4HMa8efMsSOhdf3z/Nfn8888NScaePXsaJlQDO95n8NNPPxnNmzc3vv76a6Nly5bG008/3SB5GLk5SaWlpdq0aZMGDBhQ5fiAAQO0YcMGi1JZKz8/X5IUHR1tcZKGddddd+nyyy9Xv379rI7S4N5++22lpaXp2muvVWxsrLp27aoXXnjB6lgNplevXvrwww+1Y8cOSdKXX36pdevWadCgQRYns8bu3buVk5NT5fei3W5Xnz59/Pr3os1m85vRTKlyo+phw4bp3nvvVfv27Rv0tf1u40xPy8vLk9PpVFxcXJXjcXFxysnJsSiVdQzDUHp6unr16qUOHTpYHafBvPbaa9q8ebM2btxodRRL7Nq1S3PnzlV6erruv/9+ff755xozZozsdruGDx9udTyvu++++5Sfn6+zzz5bgYGBcjqdevTRR3X99ddbHc0Srt99Nf1e3LNnjxWRLFVcXKyJEydq6NChfrWZ5uOPP66goCCNGTOmwV+bcuMhNputyveGYVQ75g9Gjx6trVu3at26dVZHaTB79+7V2LFj9f777ys0NNTqOJaoqKhQWlqapk2bJknq2rWrvvnmG82dO9cvys2yZcv06quvasmSJWrfvr22bNmicePGKTExUTfddJPV8SzD78XKxcXXXXedKioqNGfOHKvjNJhNmzbpmWee0ebNmy35b8601Elq2rSpAgMDq43S5ObmVvt/Lb7u7rvv1ttvv63Vq1frjDPOsDpOg9m0aZNyc3OVmpqqoKAgBQUFac2aNXr22WcVFBQkp9NpdUSvS0hI0DnnnFPlWEpKit8sqr/33ns1ceJEXXfdderYsaOGDRum8ePHa/r06VZHs0R8fLwk+f3vxbKyMg0ePFi7d+9WRkaGX43arF27Vrm5uWrRooX79+KePXt0zz33qFWrVl5/fcrNSQoJCVFqaqoyMjKqHM/IyFCPHj0sStWwDMPQ6NGjtXz5cq1atUrJyclWR2pQl1xyib766itt2bLF/ZWWlqYbbrhBW7ZsUWBgoNURva5nz57VTv/fsWOHWrZsaVGihlVUVKSAgKq/TgMDA336VPDaJCcnKz4+vsrvxdLSUq1Zs8Zvfi+6is3333+vDz74QDExMVZHalDDhg3T1q1bq/xeTExM1L333quVK1d6/fWZlvKA9PR0DRs2TGlpaerevbsWLFigrKwsjRo1yupoDeKuu+7SkiVL9N///leRkZHu/7fmcDgUFhZmcTrvi4yMrLa+KCIiQjExMX6z7mj8+PHq0aOHpk2bpsGDB+vzzz/XggULtGDBAqujNYgrrrhCjz76qFq0aKH27dsrMzNTM2fO1C233GJ1NK85dOiQfvjhB/f3u3fv1pYtWxQdHa0WLVpo3LhxmjZtmtq2bau2bdtq2rRpCg8P19ChQy1M7Tm1vf/ExET95S9/0ebNm/XOO+/I6XS6fy9GR0crJCTEqtgedaKfgT8WuuDgYMXHx6tdu3beD9cg52T5geeff95o2bKlERISYpx77rl+dRq0pBq/Fi1aZHU0y/jbqeCGYRj/93//Z3To0MGw2+3G2WefbSxYsMDqSA2moKDAGDt2rNGiRQsjNDTUaN26tfHAAw8YJSUlVkfzmtWrV9f4v/ubbrrJMIzK08EnT55sxMfHG3a73bjwwguNr776ytrQHlTb+9+9e/dxfy+uXr3a6ugec6KfgT9qyFPBbYZhGN6vUAAAAA2DNTcAAMCnUG4AAIBPodwAAACfQrkBAAA+hXIDAAB8CuUGAAD4FMoNAADwKZQbAADgUyg3ANwuuugijRs3zuoYboZh6K9//auio6Nls9m0ZcsWqyMBOA1QbgCcst577z299NJLeuedd5Sdne03e3V52ksvvaTGjRtbHQNoMGycCcCrnE6nbDZbtV2z62Lnzp1KSEjwm52kAXgGIzfAKeaiiy7SmDFjNGHCBEVHRys+Pl5Tpkxx3/7jjz9Wm6L57bffZLPZ9NFHH0mSPvroI9lsNq1cuVJdu3ZVWFiY+vbtq9zcXL377rtKSUlRVFSUrr/+ehUVFVV5/fLyco0ePVqNGzdWTEyM/v73v+v3W9CVlpZqwoQJat68uSIiInT++ee7X1c6Nkrwzjvv6JxzzpHdbteePXtqfK9r1qxRt27dZLfblZCQoIkTJ6q8vFySdPPNN+vuu+9WVlaWbDabWrVqddzPbP369erTp4/Cw8PVpEkTXXrppTp48KAkqaSkRGPGjFFsbKxCQ0PVq1cvbdy40f3Y+n5WF110kUaPHl3rZ3Xw4EENHz5cTZo0UXh4uAYOHKjvv/++2me1cuVKpaSkqFGjRrrsssuUnZ1d5f0tWrRIKSkpCg0N1dlnn605c+a4b3P9PCxfvlwXX3yxwsPD1blzZ33yySfu9zdixAjl5+fLZrPJZrO5f57mzJmjtm3bKjQ0VHFxcfrLX/5y3M8YOK00yPacAOqsT58+RlRUlDFlyhRjx44dxssvv2zYbDbj/fffNwzDcO84nJmZ6X7MwYMHq+w47Nqt94ILLjDWrVtnbN682WjTpo3Rp08fY8CAAcbmzZuNjz/+2IiJiTEee+yxKq/dqFEjY+zYsca3335rvPrqq0Z4eHiVHb6HDh1q9OjRw/j444+NH374wXjiiScMu91u7NixwzAMw1i0aJERHBxs9OjRw1i/fr3x7bffGocOHar2Pn/66ScjPDzcuPPOO43t27cbb775ptG0aVNj8uTJhmEYxm+//WZMnTrVOOOMM4zs7GwjNze3xs8rMzPTsNvtxh133GFs2bLF+Prrr43nnnvO2L9/v2EYhjFmzBgjMTHRWLFihfHNN98YN910k9GkSRPjwIEDXv+srrzySiMlJcX4+OOPjS1bthiXXnqp0aZNG6O0tLTKZ9WvXz9j48aNxqZNm4yUlBRj6NCh7udYsGCBkZCQYLzxxhvGrl27jDfeeMOIjo42XnrppSo/D2effbbxzjvvGN99953xl7/8xWjZsqVRVlZmlJSUGLNmzTKioqKM7OxsIzs72ygsLDQ2btxoBAYGGkuWLDF+/PFHY/PmzcYzzzxTy08mcPqg3ACnmD59+hi9evWqcuy8884z7rvvPsMwzJWbDz74wH2f6dOnG5KMnTt3uo/dfvvtxqWXXlrltVNSUoyKigr3sfvuu89ISUkxDMMwfvjhB8Nmsxk///xzlXyXXHKJMWnSJMMwKv9gSzK2bNlS6/u8//77jXbt2lV5reeff95o1KiR4XQ6DcMwjKefftpo2bJlrc9z/fXXGz179qzxtkOHDhnBwcHG4sWL3cdKS0uNxMREY8aMGYZheO+z2rFjhyHJWL9+vfv2vLw8IywszPj3v/9tGMaxz+qHH36o8hnExcW5v09KSjKWLFlS5X09/PDDRvfu3Q3DOPbz8OKLL7pv/+abbwxJxvbt292v43A4qjzHG2+8YURFRRkFBQU1fnbA6YxpKeAU1KlTpyrfJyQkKDc396SeJy4uTuHh4WrdunWVY3983gsuuEA2m839fffu3fX999/L6XRq8+bNMgxDZ511lho1auT+WrNmjXbu3Ol+TEhISLX38Efbt29X9+7dq7xWz549dejQIf300091fo9btmzRJZdcUuNtO3fuVFlZmXr27Ok+FhwcrG7dumn79u1V7uvpz2r79u0KCgrS+eef7749JiZG7dq1q/La4eHhOvPMM93f//6/9f79+7V3716NHDmyyuf9yCOPVPm8/5g/ISFBkmr9menfv79atmyp1q1ba9iwYVq8eHG1KUrgdMWCYuAUFBwcXOV7m82miooKSXIvzDV+t7ajrKzshM9js9lqfd66qKioUGBgoDZt2qTAwMAqtzVq1Mj977CwsCp/9GtiGEa1+7je04ke+3thYWG1vkZNz1fTa3v6s/r9f5/aXrum13E91vV6L7zwQpWSJKna5//H/L9/fE0iIyO1efNmffTRR3r//ff1j3/8Q1OmTNHGjRs5swqnPUZugNNMs2bNJKnKolNPXv/l008/rfZ927ZtFRgYqK5du8rpdCo3N1dt2rSp8hUfH2/qdc455xxt2LChSgnYsGGDIiMj1bx58zo/T6dOnfThhx/WeFubNm0UEhKidevWuY+VlZXpiy++UEpKiqm8NantszrnnHNUXl6uzz77zH37gQMHtGPHjjq/dlxcnJo3b65du3ZV+7yTk5PrnDMkJEROp7Pa8aCgIPXr108zZszQ1q1b9eOPP2rVqlV1fl7gVMXIDXCaCQsL0wUXXKDHHntMrVq1Ul5env7+97977Pn37t2r9PR03X777dq8ebOee+45PfXUU5Kks846SzfccIOGDx+up556Sl27dlVeXp5WrVqljh07atCgQXV+nTvvvFOzZs3S3XffrdGjR+u7777T5MmTlZ6ebuq08UmTJqljx4668847NWrUKIWEhGj16tW69tpr1bRpU91xxx269957FR0drRYtWmjGjBkqKirSyJEjTX82f1TbZ9W2bVtdddVVuu222zR//nxFRkZq4sSJat68ua666qo6v8aUKVM0ZswYRUVFaeDAgSopKdEXX3yhgwcPKj09vU7P0apVKx06dEgffvihOnfurPDwcK1atUq7du3ShRdeqCZNmmjFihWqqKhQu3bt6vVZAKcSyg1wGlq4cKFuueUWpaWlqV27dpoxY4YGDBjgkecePny4jhw5om7duikwMFB33323/vrXv7pvX7RokR555BHdc889+vnnnxUTE6Pu3bubKjaS1Lx5c61YsUL33nuvOnfurOjoaI0cOdJ0UTvrrLP0/vvv6/7771e3bt0UFham888/X9dff70k6bHHHlNFRYWGDRumwsJCpaWlaeXKlWrSpImp16lJXT6rsWPH6v/9v/+n0tJSXXjhhVqxYkW1qaja3HrrrQoPD9cTTzyhCRMmKCIiQh07djR1JekePXpo1KhRGjJkiA4cOKDJkyerX79+Wr58uaZMmaLi4mK1bdtWS5cuVfv27c18BMApyWYcb2IYAHBcF110kbp06aJZs2ZZHQXAH7DmBgAA+BTKDQAA8ClMSwEAAJ/CyA0AAPAplBsAAOBTKDcAAMCnUG4AAIBPodwAAACfQrkBAAA+hXIDAAB8CuUGAAD4lP8PpC1EHC1i/NYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f421d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62565, 15)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproduced_trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "14dad948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62565, 995)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9a1d9ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62565,)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3fecd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    #trainset_res,\n",
    "    reproduced_trainset, \n",
    "    target_res, \n",
    "    test_size=0.9, \n",
    "    random_state=42,\n",
    "    #stratify=target    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4c8e1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6256, 15)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5b990",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "143d0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7396a917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aebb7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = rfc.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2da0751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8406293842902556\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     3284155       0.58      0.70      0.63        87\n",
      "     3284156       0.91      0.34      0.49        92\n",
      "     3584204       0.78      0.80      0.79        81\n",
      "     3584210       0.82      0.55      0.66        89\n",
      "     3584225       0.38      0.48      0.42        86\n",
      "     3856283       0.47      0.64      0.55        89\n",
      "     3856634       0.70      0.54      0.61        90\n",
      "     3856718       1.00      0.70      0.82        92\n",
      "     4420908       0.60      0.39      0.47        90\n",
      "     4444696       0.71      0.70      0.71        88\n",
      "     4444725       0.90      0.93      0.92        87\n",
      "     7816483       0.93      0.76      0.84        87\n",
      "     7816502       0.54      0.50      0.52        90\n",
      "     7816558       0.79      0.16      0.27        92\n",
      "     7820678       0.79      0.67      0.72        87\n",
      "     7820846       0.60      0.68      0.64        85\n",
      "     7820887       1.00      1.00      1.00        81\n",
      "     7820921       0.94      0.77      0.85        88\n",
      "     7820958       0.53      0.52      0.53        88\n",
      "     7842329       1.00      1.00      1.00        84\n",
      "     7842391       0.69      0.82      0.75        84\n",
      "     7842410       0.69      0.72      0.70        86\n",
      "     7869560       0.71      0.56      0.63        84\n",
      "     7869595       0.45      0.73      0.56        85\n",
      "     7869606       0.57      1.00      0.73        79\n",
      "     7869697       0.58      0.71      0.64        79\n",
      "     7869706       0.58      0.47      0.52        88\n",
      "     7869740       0.61      0.46      0.52        92\n",
      "     8508566       0.64      0.59      0.61        87\n",
      "     8508593       0.82      0.92      0.86        83\n",
      "     8761851       0.86      1.00      0.93        83\n",
      "     9073248       0.68      0.73      0.70        86\n",
      "     9239057       0.73      0.43      0.54        88\n",
      "     9239078       0.55      0.29      0.38        91\n",
      "     9239081       0.92      0.52      0.66        89\n",
      "     9239087       0.71      0.95      0.81        83\n",
      "     9239187       0.64      0.69      0.66        87\n",
      "     9240416       1.00      0.96      0.98        90\n",
      "     9240441       0.65      0.63      0.64        86\n",
      "     9577176       0.60      0.66      0.62        90\n",
      "     9577186       0.67      0.75      0.71        83\n",
      "     9577260       1.00      0.62      0.77        90\n",
      "     9577288       1.00      1.00      1.00        89\n",
      "     9577421       0.87      0.61      0.72        90\n",
      "     9577427       0.73      0.62      0.67        88\n",
      "     9577471       0.63      0.88      0.74        81\n",
      "    10062599       0.73      0.74      0.73        89\n",
      "    10243236       0.56      0.91      0.69        82\n",
      "    10243260       0.83      1.00      0.91        84\n",
      "    10243281       0.65      0.53      0.58        89\n",
      "    10243287       0.46      0.51      0.48        89\n",
      "    10482230       0.75      0.46      0.57        91\n",
      "    12388540       0.76      0.57      0.65        92\n",
      "    12388555       0.85      1.00      0.92        88\n",
      "    12388594       0.73      0.94      0.82        88\n",
      "    20000000       0.64      0.54      0.59        90\n",
      "    20000001       0.84      1.00      0.91        85\n",
      "    20000002       0.75      0.64      0.69        88\n",
      "    20000003       1.00      1.00      1.00        84\n",
      "    20000004       1.00      0.70      0.82        90\n",
      "    20000005       0.99      1.00      0.99        85\n",
      "    20000006       0.96      1.00      0.98        88\n",
      "    20000007       0.52      1.00      0.68        86\n",
      "    20000008       0.79      0.86      0.82        86\n",
      "    20000009       0.91      0.60      0.72        89\n",
      "    20000010       0.78      0.49      0.61        87\n",
      "    20000011       0.61      0.74      0.67        90\n",
      "    20000012       0.85      0.57      0.68        89\n",
      "    20000013       0.90      0.78      0.83        89\n",
      "    20000014       0.88      1.00      0.94        87\n",
      "    20000015       0.79      0.60      0.68        87\n",
      "    20000016       1.00      1.00      1.00        84\n",
      "    20000017       1.00      1.00      1.00        88\n",
      "    20000018       1.00      1.00      1.00        91\n",
      "    20000019       1.00      1.00      1.00        88\n",
      "    20000020       1.00      1.00      1.00        91\n",
      "    20000021       0.82      1.00      0.90        93\n",
      "    20000022       0.94      1.00      0.97        90\n",
      "    20000023       1.00      1.00      1.00        91\n",
      "    20000024       0.93      1.00      0.96        88\n",
      "    20000025       1.00      1.00      1.00        88\n",
      "    20000026       0.71      1.00      0.83        85\n",
      "    20000027       1.00      0.92      0.96        86\n",
      "    20000028       0.96      0.93      0.95        86\n",
      "    20000029       0.78      0.70      0.74        90\n",
      "    20000030       1.00      1.00      1.00        87\n",
      "    20000031       0.75      0.58      0.65        92\n",
      "    20000032       0.94      1.00      0.97        92\n",
      "    20000033       1.00      1.00      1.00        91\n",
      "    20000034       1.00      1.00      1.00        85\n",
      "    20000035       0.93      1.00      0.97        84\n",
      "    20000036       1.00      1.00      1.00        93\n",
      "    20000037       0.97      1.00      0.98        83\n",
      "    20000038       1.00      1.00      1.00        89\n",
      "    20000039       1.00      1.00      1.00        81\n",
      "    20000040       0.99      0.80      0.88        89\n",
      "    20000041       1.00      1.00      1.00        91\n",
      "    20000042       0.92      1.00      0.96        87\n",
      "    20000043       0.97      1.00      0.98        86\n",
      "    20000044       0.82      1.00      0.90        91\n",
      "    20000045       0.97      1.00      0.98        85\n",
      "    20000046       1.00      1.00      1.00        82\n",
      "    20000047       0.41      0.66      0.50        88\n",
      "    20000048       0.47      0.40      0.43        87\n",
      "    20000049       0.76      0.82      0.79        88\n",
      "    20000050       0.51      0.24      0.33        94\n",
      "    20000051       0.91      1.00      0.95        84\n",
      "    20000052       0.94      1.00      0.97        85\n",
      "    20000053       1.00      1.00      1.00        89\n",
      "    20000054       1.00      1.00      1.00        88\n",
      "    20000055       0.49      0.77      0.60        84\n",
      "    20000056       0.84      1.00      0.91        83\n",
      "    20000057       0.56      0.20      0.30        93\n",
      "    20000058       0.99      1.00      0.99        87\n",
      "    20000059       0.78      0.32      0.46        90\n",
      "    20000060       1.00      1.00      1.00        88\n",
      "    20000061       0.85      0.70      0.76        86\n",
      "    20000062       1.00      1.00      1.00        91\n",
      "    20000063       0.91      0.94      0.92        85\n",
      "    20000064       0.58      0.84      0.69        86\n",
      "    20000065       0.56      0.37      0.45        91\n",
      "    20000066       0.98      1.00      0.99        85\n",
      "    20000067       0.39      0.56      0.46        86\n",
      "    20000068       0.92      1.00      0.96        89\n",
      "    20000069       1.00      1.00      1.00        88\n",
      "    20000070       0.95      1.00      0.97        87\n",
      "    20000071       0.83      0.81      0.82        89\n",
      "    20000072       0.75      0.60      0.67        88\n",
      "    20000073       0.65      0.88      0.75        86\n",
      "    20000074       0.75      0.66      0.70        87\n",
      "    20000075       0.37      0.17      0.23        89\n",
      "    20000076       0.79      0.73      0.76        83\n",
      "    20000077       0.78      1.00      0.88        83\n",
      "    20000078       0.61      0.15      0.24        92\n",
      "    20000079       0.51      0.44      0.47        91\n",
      "    20000080       1.00      1.00      1.00        87\n",
      "    20000081       0.98      0.48      0.65        85\n",
      "    20000082       0.75      1.00      0.86        84\n",
      "    20000083       0.98      1.00      0.99        89\n",
      "    20000084       0.91      1.00      0.95        88\n",
      "    20000085       0.99      1.00      0.99        86\n",
      "    20000086       0.73      1.00      0.85        85\n",
      "    20000087       1.00      1.00      1.00        82\n",
      "    20000088       0.79      1.00      0.89        85\n",
      "    20000089       0.76      0.68      0.72        88\n",
      "    20000090       0.87      0.83      0.85        86\n",
      "    20000091       1.00      1.00      1.00        86\n",
      "    20000092       0.81      0.70      0.75        90\n",
      "    20000093       0.66      0.94      0.78        85\n",
      "    20000094       0.88      0.70      0.78        83\n",
      "    20000095       0.93      1.00      0.97        85\n",
      "    20000096       0.93      0.60      0.73        93\n",
      "    20000097       0.95      0.88      0.92        86\n",
      "    20000098       0.90      1.00      0.95        92\n",
      "    20000099       1.00      1.00      1.00        86\n",
      "    20000100       1.00      1.00      1.00        93\n",
      "    20000101       1.00      1.00      1.00        83\n",
      "    20000102       1.00      1.00      1.00        86\n",
      "    20000103       0.80      0.81      0.80        86\n",
      "    20000104       0.76      0.92      0.84        88\n",
      "    20000105       0.60      0.63      0.61        83\n",
      "    20000106       0.73      0.37      0.49        90\n",
      "    20000107       0.79      0.87      0.83        85\n",
      "    20000108       0.73      0.84      0.78        85\n",
      "    20000109       0.66      0.54      0.59        85\n",
      "    20000110       0.55      0.91      0.69        88\n",
      "    20000111       0.91      0.61      0.73        84\n",
      "    20000112       0.42      0.31      0.36        89\n",
      "    20000113       0.53      0.55      0.54        82\n",
      "    20000114       0.93      0.90      0.91        86\n",
      "    20000115       0.75      0.88      0.81        81\n",
      "    20000116       0.44      0.71      0.54        82\n",
      "    20000117       0.74      0.39      0.51        88\n",
      "    20000118       0.89      1.00      0.94        92\n",
      "    20000119       0.41      0.39      0.40        88\n",
      "    20000120       0.91      1.00      0.95        91\n",
      "    20000121       1.00      1.00      1.00        86\n",
      "    20000122       1.00      1.00      1.00        79\n",
      "    20000123       0.74      0.74      0.74        90\n",
      "    20000124       1.00      1.00      1.00        82\n",
      "    20000125       0.53      0.56      0.54        89\n",
      "    20000126       0.82      0.67      0.74        88\n",
      "    20000127       0.52      0.79      0.63        87\n",
      "    20000128       0.83      0.54      0.65        89\n",
      "    20000129       0.87      1.00      0.93        89\n",
      "    20000130       0.67      0.75      0.71        87\n",
      "    20000131       0.38      0.51      0.44        85\n",
      "    20000132       0.57      0.85      0.68        82\n",
      "    20000133       1.00      1.00      1.00        82\n",
      "    20000134       0.60      0.74      0.66        84\n",
      "    20000135       0.91      0.47      0.62        89\n",
      "    20000136       0.85      0.96      0.91        85\n",
      "    20000137       0.65      0.72      0.68        85\n",
      "    20000138       0.46      0.62      0.52        86\n",
      "    20000139       0.48      0.68      0.56        87\n",
      "    20000140       0.40      0.27      0.33        91\n",
      "    20000141       0.83      1.00      0.91        86\n",
      "    20000142       1.00      1.00      1.00        83\n",
      "    20000143       0.65      0.86      0.74        84\n",
      "    20000144       1.00      1.00      1.00        85\n",
      "    20000145       0.92      0.73      0.81        92\n",
      "    20000146       1.00      1.00      1.00        80\n",
      "    20000147       1.00      1.00      1.00        85\n",
      "    20000148       1.00      1.00      1.00        87\n",
      "    20000149       1.00      1.00      1.00        85\n",
      "    20000150       0.64      0.29      0.40        94\n",
      "    20000151       0.61      0.69      0.64        86\n",
      "    20000152       0.71      0.68      0.70        88\n",
      "    20000153       0.97      1.00      0.98        83\n",
      "    20000154       0.56      0.69      0.62        89\n",
      "    20000155       0.54      0.57      0.55        90\n",
      "    20000156       0.59      0.59      0.59        87\n",
      "    20000157       0.92      0.77      0.84        91\n",
      "    20000158       0.45      0.28      0.34        90\n",
      "    20000159       0.88      1.00      0.94        87\n",
      "    20000160       0.78      0.83      0.80        89\n",
      "    20000161       0.92      1.00      0.96        87\n",
      "    20000162       0.53      0.59      0.55        87\n",
      "    20000163       0.68      0.87      0.77        79\n",
      "    20000164       0.72      0.62      0.67        89\n",
      "    20000165       0.81      0.78      0.79        86\n",
      "    20000166       0.78      0.61      0.68        89\n",
      "    20000167       0.96      1.00      0.98        88\n",
      "    20000168       0.98      0.66      0.79        89\n",
      "    20000169       0.76      0.80      0.78        89\n",
      "    20000170       1.00      0.80      0.89        85\n",
      "    20000171       0.92      1.00      0.96        79\n",
      "    20000172       0.68      0.54      0.60        89\n",
      "    20000173       1.00      0.25      0.40        92\n",
      "    20000174       0.79      0.74      0.77        86\n",
      "    20000175       1.00      1.00      1.00        87\n",
      "    20000176       0.88      1.00      0.94        87\n",
      "    20000177       0.70      0.83      0.76        84\n",
      "    20000178       0.75      0.81      0.78        86\n",
      "    20000179       0.92      0.55      0.69        89\n",
      "    20000180       0.81      0.79      0.80        87\n",
      "    20000181       0.90      1.00      0.95        82\n",
      "    20000182       0.93      1.00      0.97        85\n",
      "    20000183       0.76      0.66      0.71        83\n",
      "    20000184       0.98      0.74      0.85        85\n",
      "    20000185       0.91      0.95      0.93        84\n",
      "    20000186       1.00      0.66      0.80        89\n",
      "    20000187       0.60      0.62      0.61        86\n",
      "    20000188       0.79      1.00      0.88        88\n",
      "    20000189       0.88      1.00      0.93        86\n",
      "    20000190       0.75      0.81      0.78        88\n",
      "    20000191       0.82      1.00      0.90        82\n",
      "    20000192       1.00      1.00      1.00        88\n",
      "    20000193       0.86      1.00      0.93        88\n",
      "    20000194       0.92      1.00      0.96        83\n",
      "    20000195       0.85      1.00      0.92        87\n",
      "    20000196       0.90      1.00      0.95        84\n",
      "    20000197       1.00      1.00      1.00        82\n",
      "    20000198       1.00      1.00      1.00        90\n",
      "    20000199       0.86      1.00      0.92        89\n",
      "    20000200       0.93      1.00      0.96        88\n",
      "    20000201       0.95      0.82      0.88        88\n",
      "    20000202       0.84      0.93      0.88        85\n",
      "    20000203       0.99      1.00      0.99        88\n",
      "    20000204       0.85      1.00      0.92        86\n",
      "    20000205       0.85      0.84      0.85        88\n",
      "    20000206       1.00      0.83      0.91        87\n",
      "    20000207       0.95      0.81      0.88        86\n",
      "    20000208       0.74      0.74      0.74        87\n",
      "    20000209       0.99      1.00      0.99        78\n",
      "    20000210       0.82      1.00      0.90        90\n",
      "    20000211       0.76      0.43      0.55        90\n",
      "    20000212       0.65      0.65      0.65        88\n",
      "    20000213       0.91      1.00      0.95        94\n",
      "    20000214       1.00      1.00      1.00        85\n",
      "    20000215       0.97      0.86      0.91        85\n",
      "    20000216       0.68      0.63      0.65        91\n",
      "    20000217       0.68      1.00      0.81        89\n",
      "    20000218       0.89      1.00      0.94        86\n",
      "    20000219       1.00      1.00      1.00        84\n",
      "    20000220       1.00      0.77      0.87        91\n",
      "    20000221       1.00      1.00      1.00        90\n",
      "    20000222       0.95      1.00      0.97        87\n",
      "    20000223       0.67      0.83      0.74        88\n",
      "    20000224       1.00      1.00      1.00        88\n",
      "    20000225       0.96      1.00      0.98        88\n",
      "    20000226       1.00      0.89      0.94        87\n",
      "    20000227       0.87      0.45      0.59        91\n",
      "    20000228       1.00      1.00      1.00        86\n",
      "    20000229       1.00      1.00      1.00        86\n",
      "    20000230       0.86      1.00      0.92        90\n",
      "    20000231       0.96      0.86      0.91        91\n",
      "    20000232       0.70      0.60      0.65        87\n",
      "    20000233       1.00      0.83      0.91        89\n",
      "    20000234       1.00      1.00      1.00        91\n",
      "    20000235       0.58      0.53      0.55        89\n",
      "    20000236       1.00      1.00      1.00        89\n",
      "    20000237       1.00      1.00      1.00        89\n",
      "    20000238       0.88      0.80      0.84        90\n",
      "    20000239       0.90      1.00      0.95        83\n",
      "    20000240       0.68      1.00      0.81        88\n",
      "    20000241       0.97      1.00      0.98        89\n",
      "    20000242       0.64      0.72      0.68        87\n",
      "    20000243       0.86      0.78      0.81        85\n",
      "    20000244       0.68      0.79      0.73        87\n",
      "    20000245       0.82      0.82      0.82        88\n",
      "    20000246       1.00      0.93      0.97        90\n",
      "    20000247       1.00      0.72      0.84        87\n",
      "    20000248       0.82      1.00      0.90        94\n",
      "    20000249       1.00      1.00      1.00        88\n",
      "    20000250       1.00      1.00      1.00        91\n",
      "    20000251       0.78      1.00      0.87        83\n",
      "    20000252       0.79      0.71      0.75        90\n",
      "    20000253       0.87      1.00      0.93        88\n",
      "    20000254       1.00      1.00      1.00        90\n",
      "    20000255       1.00      0.92      0.96        86\n",
      "    20000256       0.98      1.00      0.99        87\n",
      "    20000257       1.00      1.00      1.00        89\n",
      "    20000258       1.00      1.00      1.00        84\n",
      "    20000259       0.93      0.87      0.90        89\n",
      "    20000260       0.92      1.00      0.96        87\n",
      "    20000261       1.00      1.00      1.00        88\n",
      "    20000262       1.00      1.00      1.00        89\n",
      "    20000263       1.00      0.90      0.95        87\n",
      "    20000264       1.00      0.29      0.45        94\n",
      "    20000265       0.82      0.77      0.80        88\n",
      "    20000266       1.00      1.00      1.00        92\n",
      "    20000267       1.00      1.00      1.00        87\n",
      "    20000268       1.00      1.00      1.00        90\n",
      "    20000269       0.90      1.00      0.95        81\n",
      "    20000270       0.78      0.83      0.80        86\n",
      "    20000271       1.00      1.00      1.00        82\n",
      "    20000272       1.00      1.00      1.00        89\n",
      "    20000273       1.00      1.00      1.00        86\n",
      "    20000274       0.76      1.00      0.86        90\n",
      "    20000275       0.89      0.74      0.81        91\n",
      "    20000276       1.00      1.00      1.00        86\n",
      "    20000277       0.91      0.91      0.91        86\n",
      "    20000278       1.00      0.91      0.95        92\n",
      "    20000279       1.00      1.00      1.00        87\n",
      "    20000280       1.00      1.00      1.00        87\n",
      "    20000281       1.00      0.95      0.98        88\n",
      "    20000282       1.00      1.00      1.00        87\n",
      "    20000283       0.89      1.00      0.94        88\n",
      "    20000284       1.00      1.00      1.00        88\n",
      "    20000285       1.00      1.00      1.00        87\n",
      "    20000286       1.00      1.00      1.00        88\n",
      "    20000287       1.00      1.00      1.00        84\n",
      "    20000288       1.00      1.00      1.00        86\n",
      "    20000289       1.00      1.00      1.00        88\n",
      "    20000290       1.00      1.00      1.00        89\n",
      "    20000291       1.00      1.00      1.00        87\n",
      "    20000292       1.00      1.00      1.00        89\n",
      "    20000293       1.00      1.00      1.00        89\n",
      "    20000294       1.00      1.00      1.00        93\n",
      "    20000295       1.00      1.00      1.00        86\n",
      "    20000296       1.00      1.00      1.00        88\n",
      "    20000297       1.00      1.00      1.00        86\n",
      "    20000298       0.95      1.00      0.98        83\n",
      "    20000299       1.00      1.00      1.00        89\n",
      "    20000300       1.00      1.00      1.00        85\n",
      "    20000301       0.87      0.95      0.91        84\n",
      "    20000302       0.76      1.00      0.86        84\n",
      "    20000303       1.00      1.00      1.00        86\n",
      "    20000304       1.00      1.00      1.00        84\n",
      "    20000305       1.00      0.74      0.85        87\n",
      "    20000306       1.00      1.00      1.00        86\n",
      "    20000307       0.65      0.71      0.68        85\n",
      "    20000308       1.00      1.00      1.00        89\n",
      "    20000309       0.96      1.00      0.98        88\n",
      "    20000310       1.00      0.62      0.76        89\n",
      "    20000311       0.72      0.85      0.78        86\n",
      "    20000312       1.00      1.00      1.00        87\n",
      "    20000313       1.00      1.00      1.00        89\n",
      "    20000314       0.64      0.61      0.63        88\n",
      "    20000315       0.92      1.00      0.96        87\n",
      "    20000316       1.00      1.00      1.00        91\n",
      "    20000317       1.00      1.00      1.00        91\n",
      "    20000318       0.75      1.00      0.85        82\n",
      "    20000319       1.00      1.00      1.00        87\n",
      "    20000320       0.98      1.00      0.99        89\n",
      "    20000321       0.37      0.62      0.46        89\n",
      "    20000322       1.00      0.49      0.66        90\n",
      "    20000323       1.00      0.45      0.62        93\n",
      "    20000324       1.00      1.00      1.00        91\n",
      "    20000325       0.68      0.65      0.66        88\n",
      "    20000326       0.97      1.00      0.98        86\n",
      "    20000327       0.75      0.82      0.78        83\n",
      "    20000328       0.63      1.00      0.77        83\n",
      "    20000329       0.87      0.64      0.73        91\n",
      "    20000330       0.80      0.74      0.77        90\n",
      "    20000331       0.95      1.00      0.97        93\n",
      "    20000332       0.88      1.00      0.94        90\n",
      "    20000333       1.00      1.00      1.00        84\n",
      "    20000334       1.00      1.00      1.00        90\n",
      "    20000335       1.00      1.00      1.00        87\n",
      "    20000336       0.96      0.70      0.81        92\n",
      "    20000337       0.91      1.00      0.95        86\n",
      "    20000338       0.93      1.00      0.97        86\n",
      "    20000339       0.80      0.41      0.54        90\n",
      "    20000340       0.91      0.80      0.85        85\n",
      "    20000341       1.00      1.00      1.00        90\n",
      "    20000342       0.85      1.00      0.92        88\n",
      "    20000343       0.46      0.64      0.53        87\n",
      "    20000344       0.49      0.49      0.49        88\n",
      "    20000345       0.58      0.92      0.71        83\n",
      "    20000346       0.63      1.00      0.77        89\n",
      "    20000347       0.72      0.37      0.49        89\n",
      "    20000348       0.63      0.61      0.62        89\n",
      "    20000349       0.86      0.86      0.86        90\n",
      "    20000350       0.89      1.00      0.94        89\n",
      "    20000351       0.69      0.53      0.60        86\n",
      "    20000352       0.60      0.91      0.72        87\n",
      "    20000353       0.78      0.93      0.85        88\n",
      "    20000354       0.73      0.78      0.76        87\n",
      "    20000355       0.96      1.00      0.98        90\n",
      "    20000356       0.97      1.00      0.98        90\n",
      "    20000357       0.86      0.90      0.88        87\n",
      "    20000358       0.88      1.00      0.94        90\n",
      "    20000359       1.00      1.00      1.00        88\n",
      "    20000360       0.94      1.00      0.97        87\n",
      "    20000361       0.98      1.00      0.99        87\n",
      "    20000362       0.56      1.00      0.71        89\n",
      "    20000363       1.00      0.72      0.84        85\n",
      "    20000364       0.79      0.49      0.60        86\n",
      "    20000365       0.83      0.76      0.79        90\n",
      "    20000366       0.42      0.44      0.43        86\n",
      "    20000367       0.81      0.53      0.64        90\n",
      "    20000368       1.00      1.00      1.00        90\n",
      "    20000369       0.57      0.52      0.55        84\n",
      "    20000370       0.81      0.66      0.73        86\n",
      "    20000371       0.92      1.00      0.96        86\n",
      "    20000372       0.55      0.83      0.66        86\n",
      "    20000373       0.73      0.58      0.65        89\n",
      "    20000374       0.73      0.64      0.68        89\n",
      "    20000375       0.76      0.41      0.53        91\n",
      "    20000376       0.81      1.00      0.90        83\n",
      "    20000377       1.00      0.93      0.96        86\n",
      "    20000378       0.68      0.50      0.58        90\n",
      "    20000379       0.64      0.58      0.61        89\n",
      "    20000380       0.47      0.80      0.59        87\n",
      "    20000381       0.76      1.00      0.86        83\n",
      "    20000382       0.95      1.00      0.97        88\n",
      "    20000383       0.95      1.00      0.97        86\n",
      "    20000384       0.76      0.75      0.75        84\n",
      "    20000385       1.00      0.80      0.89        86\n",
      "    20000386       1.00      0.97      0.98        86\n",
      "    20000387       1.00      1.00      1.00        86\n",
      "    20000388       0.80      1.00      0.89        87\n",
      "    20000389       0.71      1.00      0.83        87\n",
      "    20000390       0.60      1.00      0.75        85\n",
      "    20000391       0.94      0.51      0.66        90\n",
      "    20000392       0.88      1.00      0.94        90\n",
      "    20000393       0.67      0.53      0.59        87\n",
      "    20000394       0.85      0.83      0.84        83\n",
      "    20000395       0.86      0.71      0.78        89\n",
      "    20000396       1.00      1.00      1.00        89\n",
      "    20000397       0.84      0.87      0.86        86\n",
      "    20000398       0.54      0.48      0.51        89\n",
      "    20000399       0.53      0.72      0.61        86\n",
      "    20000400       0.87      0.81      0.84        90\n",
      "    20000401       0.42      0.34      0.37        86\n",
      "    20000402       0.61      0.71      0.66        87\n",
      "    20000403       0.58      0.49      0.53        89\n",
      "    20000404       1.00      1.00      1.00        85\n",
      "    20000405       0.64      0.97      0.77        88\n",
      "    20000406       0.56      0.43      0.49        88\n",
      "    20000407       0.86      1.00      0.92        89\n",
      "    20000408       0.81      1.00      0.90        88\n",
      "    20000409       0.77      0.64      0.70        87\n",
      "    20000410       0.59      0.57      0.58        86\n",
      "    20000411       0.81      0.71      0.75        82\n",
      "    20000412       0.76      0.80      0.78        84\n",
      "    20000413       0.84      1.00      0.91        87\n",
      "    20000414       0.75      1.00      0.85        85\n",
      "    20000415       0.86      0.86      0.86        90\n",
      "    20000416       0.91      0.95      0.93        85\n",
      "    20000417       1.00      1.00      1.00        87\n",
      "    20000418       1.00      1.00      1.00        90\n",
      "    20000419       0.89      0.42      0.57        92\n",
      "    20000420       0.54      0.58      0.56        92\n",
      "    20000421       0.69      0.91      0.78        87\n",
      "    20000422       1.00      1.00      1.00        90\n",
      "    20000423       1.00      1.00      1.00        94\n",
      "    20000424       0.90      1.00      0.95        82\n",
      "    20000425       0.60      0.61      0.61        87\n",
      "    20000426       1.00      0.49      0.66        92\n",
      "    20000427       0.65      0.38      0.48        94\n",
      "    20000428       1.00      1.00      1.00        88\n",
      "    20000429       1.00      1.00      1.00        87\n",
      "    20000430       0.84      1.00      0.91        85\n",
      "    20000431       0.39      0.52      0.45        87\n",
      "    20000432       0.91      0.73      0.81        82\n",
      "    20000433       0.88      0.91      0.89        87\n",
      "    20000434       1.00      1.00      1.00        89\n",
      "    20000435       1.00      1.00      1.00        88\n",
      "    20000436       0.64      0.73      0.69        83\n",
      "    20000437       0.88      0.72      0.79        89\n",
      "    20000438       0.85      1.00      0.92        89\n",
      "    20000439       0.66      0.49      0.56        85\n",
      "    20000440       0.91      0.84      0.87        82\n",
      "    20000441       0.82      1.00      0.90        90\n",
      "    20000442       0.77      0.75      0.76        89\n",
      "    20000443       0.67      0.75      0.71        87\n",
      "    20000444       0.95      1.00      0.97        90\n",
      "    20000445       1.00      0.43      0.60        93\n",
      "    20000446       0.92      1.00      0.96        88\n",
      "    20000447       0.99      1.00      0.99        85\n",
      "    20000448       1.00      1.00      1.00        88\n",
      "    20000449       0.95      0.43      0.60        92\n",
      "    20000450       0.95      1.00      0.97        86\n",
      "    20000451       1.00      1.00      1.00        85\n",
      "    20000452       1.00      1.00      1.00        88\n",
      "    20000453       0.49      0.69      0.57        90\n",
      "    20000454       0.89      0.90      0.89        89\n",
      "    20000455       1.00      0.85      0.92        86\n",
      "    20000456       1.00      1.00      1.00        88\n",
      "    20000457       0.79      1.00      0.88        85\n",
      "    20000458       0.90      0.87      0.88        92\n",
      "    20000459       0.88      1.00      0.94        90\n",
      "    20000460       0.86      1.00      0.92        80\n",
      "    20000461       1.00      1.00      1.00        89\n",
      "    20000462       1.00      1.00      1.00        88\n",
      "    20000463       1.00      1.00      1.00        90\n",
      "    20000464       1.00      1.00      1.00        88\n",
      "    20000465       0.68      0.49      0.57        89\n",
      "    20000466       1.00      1.00      1.00        92\n",
      "    20000467       0.95      0.80      0.87        89\n",
      "    20000468       0.76      0.67      0.71        89\n",
      "    20000469       0.80      0.85      0.83        81\n",
      "    20000470       0.56      0.36      0.44        91\n",
      "    20000471       0.81      1.00      0.90        78\n",
      "    20000472       1.00      1.00      1.00        91\n",
      "    20000473       1.00      1.00      1.00        85\n",
      "    20000474       1.00      0.70      0.82        90\n",
      "    20000475       0.72      1.00      0.83        88\n",
      "    20000476       1.00      1.00      1.00        83\n",
      "    20000477       1.00      1.00      1.00        87\n",
      "    20000478       1.00      0.52      0.69        92\n",
      "    20000479       0.98      1.00      0.99        87\n",
      "    20000480       1.00      1.00      1.00        87\n",
      "    20000481       0.68      0.71      0.69        83\n",
      "    20000482       0.80      1.00      0.89        83\n",
      "    20000483       0.87      0.83      0.85        86\n",
      "    20000484       1.00      1.00      1.00        83\n",
      "    20000485       1.00      1.00      1.00        91\n",
      "    20000486       0.88      0.41      0.56        91\n",
      "    20000487       0.92      0.95      0.94        87\n",
      "    20000488       0.82      0.87      0.85        86\n",
      "    20000489       0.74      1.00      0.85        85\n",
      "    20000490       0.93      1.00      0.96        88\n",
      "    20000491       0.88      0.94      0.91        85\n",
      "    20000492       1.00      0.86      0.93        87\n",
      "    20000493       0.65      0.43      0.52        91\n",
      "    20000494       1.00      0.23      0.37        93\n",
      "    20000495       0.92      1.00      0.96        91\n",
      "    20000496       0.60      0.75      0.67        87\n",
      "    20000497       1.00      1.00      1.00        85\n",
      "    20000498       1.00      1.00      1.00        89\n",
      "    20000499       0.96      1.00      0.98        79\n",
      "    20000500       0.77      0.93      0.84        89\n",
      "    20000501       0.55      0.49      0.52        91\n",
      "    20000502       1.00      1.00      1.00        89\n",
      "    20000503       1.00      0.57      0.72        92\n",
      "    20000504       0.73      0.74      0.74        90\n",
      "    20000505       0.57      0.87      0.69        85\n",
      "    20000506       0.95      1.00      0.97        88\n",
      "    20000507       1.00      1.00      1.00        92\n",
      "    20000508       0.97      1.00      0.98        84\n",
      "    20000509       0.86      1.00      0.92        86\n",
      "    20000510       0.83      1.00      0.91        86\n",
      "    20000511       0.85      0.93      0.89        84\n",
      "    20000512       0.66      1.00      0.80        85\n",
      "    20000513       0.88      1.00      0.94        87\n",
      "    20000514       0.98      1.00      0.99        83\n",
      "    20000515       0.79      0.80      0.80        85\n",
      "    20000516       0.75      1.00      0.86        80\n",
      "    20000517       0.90      1.00      0.95        92\n",
      "    20000518       0.88      0.57      0.69        93\n",
      "    20000519       1.00      1.00      1.00        91\n",
      "    20000520       0.86      1.00      0.93        82\n",
      "    20000521       0.92      1.00      0.96        83\n",
      "    20000522       0.83      1.00      0.90        90\n",
      "    20000523       1.00      1.00      1.00        83\n",
      "    20000524       0.85      1.00      0.92        77\n",
      "    20000525       0.73      0.64      0.69        90\n",
      "    20000526       0.72      1.00      0.84        87\n",
      "    20000527       0.82      1.00      0.90        91\n",
      "    20000528       1.00      1.00      1.00        89\n",
      "    20000529       0.93      0.88      0.90        86\n",
      "    20000530       0.62      0.46      0.53        90\n",
      "    20000531       0.97      1.00      0.98        89\n",
      "    20000532       1.00      1.00      1.00        90\n",
      "    20000533       0.95      0.69      0.80        87\n",
      "    20000534       0.83      0.58      0.68        92\n",
      "    20000535       1.00      1.00      1.00        88\n",
      "    20000536       1.00      1.00      1.00        93\n",
      "    20000537       0.93      1.00      0.97        84\n",
      "    20000538       1.00      1.00      1.00        86\n",
      "    20000539       1.00      1.00      1.00        89\n",
      "    20000540       1.00      1.00      1.00        86\n",
      "    20000541       1.00      0.92      0.96        87\n",
      "    20000542       0.87      0.76      0.81        91\n",
      "    20000543       0.92      0.51      0.65        87\n",
      "    20000544       0.85      0.59      0.70        90\n",
      "    20000545       1.00      1.00      1.00        91\n",
      "    20000546       0.91      1.00      0.96        86\n",
      "    20000547       0.80      0.78      0.79        85\n",
      "    20000548       0.96      0.90      0.93        88\n",
      "    20000549       0.91      1.00      0.95        90\n",
      "    20000550       1.00      1.00      1.00        84\n",
      "    20000551       0.93      1.00      0.96        93\n",
      "    20000552       0.74      0.67      0.70        88\n",
      "    20000553       0.94      1.00      0.97        81\n",
      "    20000554       1.00      1.00      1.00        89\n",
      "    20000555       1.00      1.00      1.00        88\n",
      "    20000556       0.92      0.85      0.88        86\n",
      "    20000557       1.00      1.00      1.00        87\n",
      "    20000558       1.00      1.00      1.00        88\n",
      "    20000559       1.00      1.00      1.00        86\n",
      "    20000560       0.76      0.65      0.70        89\n",
      "    20000561       1.00      1.00      1.00        89\n",
      "    20000562       0.94      1.00      0.97        92\n",
      "    20000563       1.00      1.00      1.00        79\n",
      "    20000564       1.00      1.00      1.00        86\n",
      "    20000565       1.00      1.00      1.00        85\n",
      "    20000566       0.80      1.00      0.89        83\n",
      "    20000567       1.00      1.00      1.00        90\n",
      "    20000568       1.00      1.00      1.00        85\n",
      "    20000569       1.00      1.00      1.00        86\n",
      "    20000570       1.00      1.00      1.00        89\n",
      "    20000571       1.00      1.00      1.00        90\n",
      "    20000572       1.00      1.00      1.00        84\n",
      "    20000573       1.00      0.73      0.85        86\n",
      "    20000574       1.00      1.00      1.00        83\n",
      "    20000575       1.00      1.00      1.00        88\n",
      "    20000576       1.00      1.00      1.00        85\n",
      "    20000577       1.00      1.00      1.00        89\n",
      "    20000578       1.00      1.00      1.00        84\n",
      "    20000579       0.96      1.00      0.98        86\n",
      "    20000580       1.00      1.00      1.00        84\n",
      "    20000581       1.00      1.00      1.00        92\n",
      "    20000582       1.00      1.00      1.00        86\n",
      "    20000583       1.00      1.00      1.00        92\n",
      "    20000584       1.00      1.00      1.00        86\n",
      "    20000585       1.00      1.00      1.00        85\n",
      "    20000586       1.00      1.00      1.00        85\n",
      "    20000587       0.97      1.00      0.98        89\n",
      "    20000588       1.00      1.00      1.00        92\n",
      "    20000589       1.00      1.00      1.00        87\n",
      "\n",
      "    accuracy                           0.84     56309\n",
      "   macro avg       0.85      0.84      0.84     56309\n",
      "weighted avg       0.85      0.84      0.83     56309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "report = classification_report(test_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7be5b",
   "metadata": {},
   "source": [
    "#### Обучим модель CatBoostClassifier на подготовленных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a1443d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c000462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 5.8021328\ttest: 5.8028781\tbest: 5.8028781 (0)\ttotal: 4.36s\tremaining: 1h 12m 36s\n",
      "1:\tlearn: 5.7600391\ttest: 5.7615252\tbest: 5.7615252 (1)\ttotal: 8.73s\tremaining: 1h 12m 36s\n",
      "2:\tlearn: 5.7070225\ttest: 5.7088004\tbest: 5.7088004 (2)\ttotal: 13.3s\tremaining: 1h 13m 40s\n",
      "3:\tlearn: 5.6601069\ttest: 5.6619228\tbest: 5.6619228 (3)\ttotal: 17.8s\tremaining: 1h 13m 55s\n",
      "4:\tlearn: 5.6294886\ttest: 5.6320792\tbest: 5.6320792 (4)\ttotal: 22.2s\tremaining: 1h 13m 40s\n",
      "5:\tlearn: 5.5898019\ttest: 5.5922128\tbest: 5.5922128 (5)\ttotal: 26.7s\tremaining: 1h 13m 44s\n",
      "6:\tlearn: 5.5461853\ttest: 5.5487469\tbest: 5.5487469 (6)\ttotal: 31.3s\tremaining: 1h 13m 58s\n",
      "7:\tlearn: 5.5087435\ttest: 5.5118871\tbest: 5.5118871 (7)\ttotal: 35.8s\tremaining: 1h 14m 4s\n",
      "8:\tlearn: 5.4728678\ttest: 5.4764603\tbest: 5.4764603 (8)\ttotal: 40.5s\tremaining: 1h 14m 24s\n",
      "9:\tlearn: 5.4400767\ttest: 5.4432583\tbest: 5.4432583 (9)\ttotal: 45.3s\tremaining: 1h 14m 42s\n",
      "10:\tlearn: 5.3952072\ttest: 5.3984957\tbest: 5.3984957 (10)\ttotal: 50s\tremaining: 1h 14m 52s\n",
      "11:\tlearn: 5.3746788\ttest: 5.3780192\tbest: 5.3780192 (11)\ttotal: 54.8s\tremaining: 1h 15m 10s\n",
      "12:\tlearn: 5.3439429\ttest: 5.3478626\tbest: 5.3478626 (12)\ttotal: 59s\tremaining: 1h 14m 36s\n",
      "13:\tlearn: 5.3019149\ttest: 5.3059202\tbest: 5.3059202 (13)\ttotal: 1m 4s\tremaining: 1h 15m 38s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\qlever\\qlever_ml_classification\\TNVED_classification.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m grid_small \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m],\u001b[39m# 0.03, 0.09],\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdepth\u001b[39m\u001b[39m'\u001b[39m: [ \u001b[39m3\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m#'l2_leaf_reg': [1, 3, 5, 7, 9]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m grid_full \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m, \u001b[39m0.03\u001b[39m, \u001b[39m0.05\u001b[39m, \u001b[39m0.07\u001b[39m, \u001b[39m0.09\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdepth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m10\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m#'early_stopping_rounds': True,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39ml2_leaf_reg\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m9\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m grid_search_result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgrid_search(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         grid_small,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         X\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         y\u001b[39m=\u001b[39;49mtrain_labels,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n",
      "File \u001b[1;32mg:\\Apps\\anaconda3\\envs\\qlever\\Lib\\site-packages\\catboost\\core.py:4124\u001b[0m, in \u001b[0;36mCatBoost.grid_search\u001b[1;34m(self, param_grid, X, y, cv, partition_random_seed, calc_cv_statistics, search_by_train_test_split, refit, shuffle, stratified, train_size, verbose, plot, plot_file, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   4121\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(grid[key], Iterable):\n\u001b[0;32m   4122\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mParameter grid value is not iterable (key=\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m, value=\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(key, grid[key]))\n\u001b[1;32m-> 4124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tune_hyperparams(\n\u001b[0;32m   4125\u001b[0m     param_grid\u001b[39m=\u001b[39;49mparam_grid, X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, cv\u001b[39m=\u001b[39;49mcv, n_iter\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   4126\u001b[0m     partition_random_seed\u001b[39m=\u001b[39;49mpartition_random_seed, calc_cv_statistics\u001b[39m=\u001b[39;49mcalc_cv_statistics,\n\u001b[0;32m   4127\u001b[0m     search_by_train_test_split\u001b[39m=\u001b[39;49msearch_by_train_test_split, refit\u001b[39m=\u001b[39;49mrefit, shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   4128\u001b[0m     stratified\u001b[39m=\u001b[39;49mstratified, train_size\u001b[39m=\u001b[39;49mtrain_size, verbose\u001b[39m=\u001b[39;49mverbose, plot\u001b[39m=\u001b[39;49mplot, plot_file\u001b[39m=\u001b[39;49mplot_file,\n\u001b[0;32m   4129\u001b[0m     log_cout\u001b[39m=\u001b[39;49mlog_cout, log_cerr\u001b[39m=\u001b[39;49mlog_cerr,\n\u001b[0;32m   4130\u001b[0m )\n",
      "File \u001b[1;32mg:\\Apps\\anaconda3\\envs\\qlever\\Lib\\site-packages\\catboost\\core.py:4018\u001b[0m, in \u001b[0;36mCatBoost._tune_hyperparams\u001b[1;34m(self, param_grid, X, y, cv, n_iter, partition_random_seed, calc_cv_statistics, search_by_train_test_split, refit, shuffle, stratified, train_size, verbose, plot, plot_file, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   4015\u001b[0m     stratified \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(loss_function, STRING_TYPES) \u001b[39mand\u001b[39;00m is_cv_stratified_objective(loss_function)\n\u001b[0;32m   4017\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mHyperparameters search plot\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(params)]):\n\u001b[1;32m-> 4018\u001b[0m     cv_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_tune_hyperparams(\n\u001b[0;32m   4019\u001b[0m         param_grid, train_params[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_pool\u001b[39;49m\u001b[39m\"\u001b[39;49m], params, n_iter,\n\u001b[0;32m   4020\u001b[0m         fold_count, partition_random_seed, shuffle, stratified, train_size,\n\u001b[0;32m   4021\u001b[0m         search_by_train_test_split, calc_cv_statistics, custom_folds, verbose\n\u001b[0;32m   4022\u001b[0m     )\n\u001b[0;32m   4024\u001b[0m \u001b[39mif\u001b[39;00m refit:\n\u001b[0;32m   4025\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fitted()\n",
      "File \u001b[1;32m_catboost.pyx:5128\u001b[0m, in \u001b[0;36m_catboost._CatBoost._tune_hyperparams\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5166\u001b[0m, in \u001b[0;36m_catboost._CatBoost._tune_hyperparams\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(loss_function='MultiClass', logging_level='Verbose')\n",
    "\n",
    "#Нужно контролить переобоучение, правильно выставить learning_rate\n",
    "\n",
    "grid_small = {'learning_rate': [0.01],# 0.03, 0.09],\n",
    "        'depth': [ 3]\n",
    "        #'l2_leaf_reg': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "\n",
    "\n",
    "grid_full = {'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.09],\n",
    "        'depth': [3, 4, 6, 7, 8, 9, 10],\n",
    "        #'early_stopping_rounds': True,\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "\n",
    "grid_search_result = model.grid_search(\n",
    "        grid_small,\n",
    "        X=train_data,\n",
    "        y=train_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels)\n",
    "preds_labels = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "25a403ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 4.4773927\ttotal: 27.6s\tremaining: 7h 40m 22s\n",
      "1:\tlearn: 4.1295189\ttotal: 54.8s\tremaining: 7h 35m 20s\n",
      "2:\tlearn: 3.5977834\ttotal: 1m 21s\tremaining: 7h 33m 10s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-b4b3f2764b55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Нужно контролить переобоучение, правильно выставить learning_rate, метрика и функция потерь\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpreds_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5098\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_function'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5100\u001b[1;33m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[0;32m   5101\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5102\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2317\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_cout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_cerr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2318\u001b[0m             \u001b[0mplot_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Training plots'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2319\u001b[1;33m             self._train(\n\u001b[0m\u001b[0;32m   2320\u001b[0m                 \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2321\u001b[0m                 \u001b[0mtrain_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eval_sets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1723\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1724\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_2 = CatBoostClassifier(loss_function='MultiClass', logging_level='Verbose', learning_rate = 0.3, depth = 4)\n",
    "\n",
    "#Нужно контролить переобоучение, правильно выставить learning_rate, метрика и функция потерь\n",
    "model_2.fit(train_data, train_labels)\n",
    "preds_labels = model_2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1acf3d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [18770, 437]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-81b4ed1a1848>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                     )\n\u001b[0;32m    210\u001b[0m                 ):\n\u001b[1;32m--> 211\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multilabel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_pred\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    408\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [18770, 437]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(test_labels, preds_labels)\n",
    "report = classification_report(test_labels, preds_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709817bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa65c4b",
   "metadata": {},
   "source": [
    "#### Удалим бесполезные факторы которые состоят из 1 уникального значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Делаем это после кодирования категориалььных переменных\n",
    "#for column in factors_df.columns:\n",
    "#    unique_values_count = factors_df[column].drop_duplicates().size\n",
    "#    if unique_values_count == 1:\n",
    "#        new_factors_df = factors_df.drop(column, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_factors_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_factors_df['Наименование терминального класса'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2902a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e819d83",
   "metadata": {},
   "source": [
    "* Удалить колонки для неполного соответствия: если есть [1,2,3,4] - удалить одну, чтобы не было зависимости\n",
    "* Удалить строки без характеристик\n",
    "* Отбор на основе важности признаков"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
