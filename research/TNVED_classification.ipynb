{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7a424c",
   "metadata": {},
   "source": [
    "## Замечания:\n",
    "Актуально:\n",
    "* С дисбалансом из-за полупустых классов можно бороться либо удалением маленьких классов, либо **пересемплированием**. Пока выбираю второй вариант, но в любом случае чудес ждать от предсказания маленьких по объему классов не стоит - если в обучающей выборке есть 1 строка какого-то класса, значит он будет предсказывать этот класс исключительно при полном соответствии тестовых строк данной одной строке. \n",
    "*\n",
    "\n",
    "\n",
    "Актуально для меня:\n",
    "* Можно улучшить обработку текстовых данных (можно попробовать в сыром виде запихать в кэтбуст)\n",
    "* Если лемматизация будет работать долго (есть такая вероятность) - можно поменять на стеммер\n",
    "\n",
    "Старое:\n",
    "* Время работы? - **нужно протестировать на больших датасетах**\n",
    "* Полупустые классы из-за которых возникает дисбаланс - **бороться**\n",
    "\n",
    "* Вариант с тем, что в одной колонке название характеристики, в другой значение будет работать плохо (вот пример), т.к. модели без разницы на порядок следования колонок\n",
    "* Правильно ли я понимаю, что все колонки кроме ХК 1 и целевой имеют тип данных String?\n",
    "* **Как предсказывать строки с пустыми значениями во всех колонках ХК? (может их сразу откидывать)?**\n",
    "* Названия первой колонки должны быть всегда одинаковые\n",
    "* !!!Важно!!! Будем заменять числовые факторы на категориальные, если в них маленькое количество уникальных значений или одно значение встречается очень часто\n",
    "* Были ошибки в названиях колонок: 'ХК_ка т_01'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d3e75-a660-479c-a05e-2dab064987f0",
   "metadata": {},
   "source": [
    "**Проблемы, решение которых нужно будет автоматизировать:**\n",
    "* Несбалансированность классов\n",
    "* Пропуски в данных\n",
    "* Автоматическое кодирование текстовых столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "10e9e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install loguru\n",
    "#!pip install imblearn\n",
    "#!pip install pymystem3\n",
    "#!pip install catboost\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "04316d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a94931",
   "metadata": {},
   "source": [
    "#### Глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6121d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Максимальное количество уникальных значений для категориального фактора, при котором он может обрабатываться методом one-hot encoding (добавится максимум столько столбцов)\n",
    "OneHotEncodingLimit = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b4d29",
   "metadata": {},
   "source": [
    "### Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6d87f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/tire_classificator_data.xlsx')\n",
    "#df = pd.read_excel('data/paper_classificator_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ac541b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop('Историческое наименование', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e945fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ID класса (ТАРГЕТ)'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b81bd7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID класса (ТАРГЕТ)</th>\n",
       "      <th>Наименование терминального класса</th>\n",
       "      <th>Код родительского класса</th>\n",
       "      <th>Наименование родительского класса</th>\n",
       "      <th>Историческое наименование</th>\n",
       "      <th>ХК_Кат_01</th>\n",
       "      <th>Значение ХК_Кат_01</th>\n",
       "      <th>ХК_Кат_02</th>\n",
       "      <th>Значение ХК_Кат_02</th>\n",
       "      <th>ХК_Кат_03</th>\n",
       "      <th>Значение ХК_Кат_03</th>\n",
       "      <th>ХК_Кат_04</th>\n",
       "      <th>Значение ХК_Кат_04</th>\n",
       "      <th>ХК_Кат_05</th>\n",
       "      <th>Значение ХК_Кат_05</th>\n",
       "      <th>ХК_Стр_01</th>\n",
       "      <th>Значение ХК_Стр_01</th>\n",
       "      <th>ХК_Булево_01</th>\n",
       "      <th>Значение ХК_Булево_01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12388594</td>\n",
       "      <td>Шины автомобильные зимние 215/75 R16</td>\n",
       "      <td>01.09.08.01</td>\n",
       "      <td>ШИНЫ АВТОМОБИЛЬНЫЕ</td>\n",
       "      <td>Шины автомобильные зимние 215/75, R16, 116/114...</td>\n",
       "      <td>Производитель</td>\n",
       "      <td>GoodYear</td>\n",
       "      <td>Диаметр</td>\n",
       "      <td>R16</td>\n",
       "      <td>Размерность</td>\n",
       "      <td>215/75</td>\n",
       "      <td>Сезоность</td>\n",
       "      <td>зимние</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID класса (ТАРГЕТ)     Наименование терминального класса  \\\n",
       "0            12388594  Шины автомобильные зимние 215/75 R16   \n",
       "\n",
       "  Код родительского класса Наименование родительского класса  \\\n",
       "0              01.09.08.01                ШИНЫ АВТОМОБИЛЬНЫЕ   \n",
       "\n",
       "                           Историческое наименование      ХК_Кат_01  \\\n",
       "0  Шины автомобильные зимние 215/75, R16, 116/114...  Производитель   \n",
       "\n",
       "  Значение ХК_Кат_01 ХК_Кат_02 Значение ХК_Кат_02    ХК_Кат_03  \\\n",
       "0           GoodYear   Диаметр                R16  Размерность   \n",
       "\n",
       "  Значение ХК_Кат_03  ХК_Кат_04 Значение ХК_Кат_04 ХК_Кат_05  \\\n",
       "0             215/75  Сезоность             зимние       NaN   \n",
       "\n",
       "  Значение ХК_Кат_05 ХК_Стр_01 Значение ХК_Стр_01 ХК_Булево_01  \\\n",
       "0                NaN       NaN                NaN          NaN   \n",
       "\n",
       "  Значение ХК_Булево_01  \n",
       "0                   NaN  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f9af1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['ID класса (ТАРГЕТ)']\n",
    "\n",
    "# Удалим все лишние текстовые столбцы кроме \"Исторического наименования\"\n",
    "trainset_columns = []\n",
    "for column in df.columns:\n",
    "    if (column == 'Историческое наименование') or (re.fullmatch(r'ХК_.*', column)!=None) or (re.fullmatch(r'Значение.*', column)!=None):\n",
    "        trainset_columns.append(column)\n",
    "\n",
    "factors_df = df[trainset_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2355b",
   "metadata": {},
   "source": [
    "#### Работаем с типами данных столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0cee265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_column_types(columns: list):\n",
    "    '''\n",
    "    Обрабатывает названия колонок из массива columns.\n",
    "    Возвращает словарь с парами: название колонки - ее тип данных  \n",
    "    '''\n",
    "    feature_types_dict = {}\n",
    "    for column in columns:\n",
    "        type_pattern = r'ХК_([^_]+)_.*'\n",
    "        if column[0:2] == 'ХК':\n",
    "            feature_types_dict[column] = 'Кат'\n",
    "        elif column[0:8]=='Значение':\n",
    "            column_type = re.findall(type_pattern, column)[0]\n",
    "            feature_types_dict[column] = column_type\n",
    "        else:\n",
    "            feature_types_dict[column] = 'Стр'\n",
    "    return feature_types_dict\n",
    "\n",
    "feature_types_dict = format_column_types(factors_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "79196d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4592\\1022614043.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  factors_df[feature] = factors_df_copy[feature].astype(object)\n",
      "2023-10-16 15:47:11.240 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_01\n",
      "2023-10-16 15:47:11.241 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_01\n",
      "2023-10-16 15:47:11.241 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_02\n",
      "2023-10-16 15:47:11.242 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_02\n",
      "2023-10-16 15:47:11.242 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_03\n",
      "2023-10-16 15:47:11.243 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_03\n",
      "2023-10-16 15:47:11.245 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_04\n",
      "2023-10-16 15:47:11.246 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_04\n",
      "2023-10-16 15:47:11.246 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Кат_05\n",
      "2023-10-16 15:47:11.247 | DEBUG    | __main__:<module>:33 - Категориальный фактор: Значение ХК_Кат_05\n",
      "2023-10-16 15:47:11.247 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Стр_01\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4592\\1022614043.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  factors_df[feature] = factors_df_copy[feature].astype(object)\n",
      "2023-10-16 15:47:11.253 | DEBUG    | __main__:<module>:33 - Категориальный фактор: ХК_Булево_01\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4592\\1022614043.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  factors_df[feature] = factors_df_copy[feature].astype(bool)\n"
     ]
    }
   ],
   "source": [
    "def check_number_to_categorical(column: str, factor: pd.Series):\n",
    "    logger.info(f'Начинаем проверку численного фактора {column}\\n')\n",
    "    logger.debug(f'Размер фактора:{factor.size}')\n",
    "    logger.debug(f'Количество уникальных значений: {factor.drop_duplicates().size}')\n",
    "    logger.debug(f'Процент заполненности фактора: {factor[factor.notnull()].size / factor.size * 100}%')\n",
    "    popular_value = pd.DataFrame(factor.value_counts().sort_values(ascending=False).head(1)/factor[factor.notnull()].size*100)\n",
    "    popular_value.columns = ['Частота']\n",
    "    logger.debug(f'Cамое частое значение фактора: \\n{popular_value}')\n",
    "    \n",
    "    if float(popular_value.iloc[0])>=50:\n",
    "        logger.info(f'Переводим числовой фактор {column} в категориальный')\n",
    "        return True\n",
    "    \n",
    "    \n",
    "for feature in feature_types_dict.keys():\n",
    "    if feature_types_dict.get(feature) == 'Стр':\n",
    "        #logger.debug(f'Строковый фактор: {feature}')\n",
    "        #todo ДОДЕЛАТЬ пока ничего не делаем, чтобы не потерять пропущенные значения при преобразовании в строковый формат\n",
    "        factors_df_copy = factors_df.copy()\n",
    "        factors_df[feature] = factors_df_copy[feature].astype(object)\n",
    "    elif feature_types_dict.get(feature) == 'Булево':\n",
    "        factors_df_copy = factors_df.copy()\n",
    "        factors_df[feature] = factors_df_copy[feature].astype(bool)\n",
    "    elif feature_types_dict.get(feature) == 'Числ':\n",
    "        if check_number_to_categorical(feature, factors_df[feature]):\n",
    "            feature_types_dict[feature] = 'Кат'\n",
    "            factors_df_copy = factors_df.copy()\n",
    "            factors_df[feature] = factors_df_copy[feature].astype(object)\n",
    "        else:\n",
    "            factors_df_copy = factors_df.copy()\n",
    "            factors_df[feature] = factors_df_copy[feature].astype(float)\n",
    "    elif feature_types_dict.get(feature) == 'Кат':\n",
    "        logger.debug(f'Категориальный фактор: {feature}')\n",
    "        #todo ДОДЕЛАТЬ преобразование категориальных колонок (пока не делаем, т.к. возможно будет catboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5a9e5",
   "metadata": {},
   "source": [
    "#### Заполняем пропуски в данных\n",
    "В зависимости от типа данных колонки заполняем пропуски по-разному:\n",
    "*   Стр -  т.к. переводим строки в числа, то пропущенные значение пусть будут = 0\n",
    "*   Числ - #todo По умолчанию = 0. Если присутствует значение, количество которого в заполненных строках >=50% => то фактор станет категориальным, а не численным. \n",
    "*   Булево - #todo будем считать, что у нас всегда такие столбцы отвечают на вопрос: \"Есть что-то? - Да/Нет\". Если нет ответа => Нет\n",
    "*   Кат - 'Emptyclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "5af29dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Историческое наименование     0\n",
       "ХК_Кат_01                     0\n",
       "Значение ХК_Кат_01            0\n",
       "ХК_Кат_02                     0\n",
       "Значение ХК_Кат_02            0\n",
       "ХК_Кат_03                     0\n",
       "Значение ХК_Кат_03            0\n",
       "ХК_Кат_04                     0\n",
       "Значение ХК_Кат_04            0\n",
       "ХК_Кат_05                    40\n",
       "Значение ХК_Кат_05           40\n",
       "ХК_Стр_01                    49\n",
       "Значение ХК_Стр_01           49\n",
       "ХК_Булево_01                 40\n",
       "Значение ХК_Булево_01         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "3fcd845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_empty_factors_df = factors_df.copy()\n",
    "for column in not_empty_factors_df.columns:\n",
    "    if feature_types_dict.get(column) == 'Кат':\n",
    "        not_empty_factors_df.loc[not_empty_factors_df[column].isna(), column] = f'EmptyCat'\n",
    "    elif feature_types_dict.get(column) == 'Числ':\n",
    "        not_empty_factors_df.loc[not_empty_factors_df[column].isna(), column] = 0\n",
    "    elif feature_types_dict.get(column) == 'Стр':\n",
    "        not_empty_factors_df.loc[not_empty_factors_df[column].isna(), column] = ''\n",
    "    elif feature_types_dict.get(column) == 'Булево':\n",
    "        not_empty_factors_df.loc[not_empty_factors_df[column].isna(), column] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "06b664ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Историческое наименование    9151\n",
       "ХК_Кат_01                    9151\n",
       "Значение ХК_Кат_01           9151\n",
       "ХК_Кат_02                    9151\n",
       "Значение ХК_Кат_02           9151\n",
       "ХК_Кат_03                    9151\n",
       "Значение ХК_Кат_03           9151\n",
       "ХК_Кат_04                    9151\n",
       "Значение ХК_Кат_04           9151\n",
       "ХК_Кат_05                    9151\n",
       "Значение ХК_Кат_05           9151\n",
       "ХК_Стр_01                    9151\n",
       "Значение ХК_Стр_01           9151\n",
       "ХК_Булево_01                 9151\n",
       "Значение ХК_Булево_01        9151\n",
       "dtype: int64"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_empty_factors_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ae2be",
   "metadata": {},
   "source": [
    "#### Кодируем переменные\n",
    "* bag_of_words - пока остановимся на нем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "406e478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "mystem = Mystem()\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    tokens = mystem.lemmatize(text)\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords and token != \" \"  and token.strip() not in punctuation]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def text_feature_preprocessing(text_feature):\n",
    "    '''\n",
    "    Функция преобразования текстовых факторов\n",
    "    - Переводим в нижний регистр\n",
    "    - Удаляем знаки препинания\n",
    "    - Удаляем стоп слова\n",
    "    - Проводим лемматизацию\n",
    "    '''\n",
    "    processed_feature = []\n",
    "    text_feature = text_feature.replace(r'[^\\w\\s]',' ', regex=True).replace(r'\\s+',' ', regex=True).str.lower()\n",
    "    #processed_text_feature = text_feature.apply(text_preprocessing)\n",
    "    #return processed_text_feature\n",
    "    return text_feature\n",
    "\n",
    "def handle_text_feature(text_feature: pd.Series):\n",
    "    '''\n",
    "    Функция обработки строкового фактора:\n",
    "    - Проводим препроцессинг\n",
    "    - Формируем \"Мешок строк\" (bag of words)\n",
    "    '''\n",
    "    #text_feature = text_feature.drop_duplicates()\n",
    "    processed_text_feature = text_feature_preprocessing(text_feature)\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(processed_text_feature)\n",
    "    vectorized_text_feature = pd.DataFrame(vectorizer.transform(processed_text_feature).toarray())\n",
    "\n",
    "    # Удалим неинформативные столбцы\n",
    "    informative_word_columns = vectorized_text_feature.sum()[\n",
    "            (vectorized_text_feature.sum()>=vectorized_text_feature.shape[1]*0.01) &\n",
    "            (vectorized_text_feature.sum()!=vectorized_text_feature.shape[1])\n",
    "        ].index \n",
    "    handled_text_feature = vectorized_text_feature[informative_word_columns]\n",
    "    handled_text_feature = handled_text_feature.fillna('EmptyStr')\n",
    "    handled_text_feature.columns = pd.Series(vectorizer.get_feature_names_out())[informative_word_columns]\n",
    "    return handled_text_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7c36d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def handle_cat_feature(cat_feature: pd.Series):\n",
    "    cat_feature = cat_feature.astype(str)\n",
    "    unique_values_count = cat_feature.drop_duplicates().size\n",
    "    if unique_values_count <= OneHotEncodingLimit:\n",
    "        #OneHotEncoding\n",
    "        cat_feature_encoded = pd.get_dummies(cat_feature)\n",
    "    else:\n",
    "        #LabelEncoding - чтобы сильно не увеличивать количество факторов\n",
    "        le = LabelEncoder()\n",
    "        cat_feature_encoded = pd.DataFrame(le.fit_transform(cat_feature))\n",
    "    return cat_feature_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f53ad5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.DataFrame()\n",
    "copy_df = not_empty_factors_df.copy()\n",
    "\n",
    "for feature in feature_types_dict:\n",
    "    if feature_types_dict.get(feature) == 'Стр':\n",
    "        handled_feature = handle_text_feature(not_empty_factors_df[feature])\n",
    "        handled_feature = handled_feature.fillna('')\n",
    "        #handled_feature = handle_text_feature(copy_df[feature])\n",
    "    elif feature_types_dict.get(feature) == 'Кат':\n",
    "        handled_feature = handle_cat_feature(not_empty_factors_df[feature])\n",
    "        #handled_feature = handle_cat_feature(copy_df[feature])\n",
    "    else:\n",
    "        handled_feature = pd.DataFrame(not_empty_factors_df[feature])\n",
    "        #handled_feature = pd.DataFrame(copy_df[feature])\n",
    "    handled_feature.columns = [str(col)+'_'+str(feature) for col in handled_feature.columns]  \n",
    "    trainset = pd.concat([trainset,handled_feature],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9baf2f",
   "metadata": {},
   "source": [
    "#### Сэмплируем классы, в которых всего 1 экземпляр и делим на обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "db43b13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID класса (ТАРГЕТ)\n",
       "20000075    97\n",
       "20000109    91\n",
       "3584225     87\n",
       "20000112    85\n",
       "20000343    80\n",
       "            ..\n",
       "20000203     1\n",
       "20000280     1\n",
       "20000249     1\n",
       "20000582     1\n",
       "20000561     1\n",
       "Name: count, Length: 645, dtype: int64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "abc36992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID класса (ТАРГЕТ)\n",
       "12388594    97\n",
       "20000029    97\n",
       "20000234    97\n",
       "20000214    97\n",
       "20000069    97\n",
       "            ..\n",
       "20000479    97\n",
       "20000511    97\n",
       "20000542    97\n",
       "20000587    97\n",
       "20000561    97\n",
       "Name: count, Length: 645, dtype: int64"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "trainset_res, target_res = ros.fit_resample(trainset, target)\n",
    "target_res.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004a332",
   "metadata": {},
   "source": [
    "#### Уменьшаем размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "fe2c6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9ce22c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 15)\n",
    "reproduced_trainset = pca.fit_transform(trainset_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "f6f25998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2f421d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62565, 15)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reproduced_trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "14dad948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62565, 995)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9a1d9ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62565,)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3fecd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    reproduced_trainset, \n",
    "    target_res, \n",
    "    test_size=0.5, \n",
    "    random_state=42,\n",
    "    #stratify=target    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c4c8e1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31282, 15)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5b990",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "143d0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7396a917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "aebb7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = rfc.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2da0751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.985934852795448\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     3284155       0.90      0.96      0.93        47\n",
      "     3284156       0.84      0.81      0.83        47\n",
      "     3584204       1.00      0.96      0.98        48\n",
      "     3584210       1.00      1.00      1.00        47\n",
      "     3584225       0.67      0.78      0.72        50\n",
      "     3856283       0.82      1.00      0.90        40\n",
      "     3856634       1.00      1.00      1.00        55\n",
      "     3856718       1.00      1.00      1.00        49\n",
      "     4420908       0.90      0.98      0.94        48\n",
      "     4444696       0.96      1.00      0.98        49\n",
      "     4444725       0.94      1.00      0.97        50\n",
      "     7816483       1.00      1.00      1.00        47\n",
      "     7816502       1.00      0.86      0.93        51\n",
      "     7816558       0.92      0.92      0.92        48\n",
      "     7820678       0.94      0.90      0.92        49\n",
      "     7820846       1.00      0.88      0.94        50\n",
      "     7820887       1.00      1.00      1.00        40\n",
      "     7820921       1.00      1.00      1.00        48\n",
      "     7820958       0.90      0.96      0.93        46\n",
      "     7842329       1.00      1.00      1.00        48\n",
      "     7842391       0.90      0.92      0.91        48\n",
      "     7842410       1.00      1.00      1.00        48\n",
      "     7869560       1.00      0.89      0.94        47\n",
      "     7869595       0.93      0.93      0.93        46\n",
      "     7869606       0.98      1.00      0.99        41\n",
      "     7869697       0.93      0.89      0.91        47\n",
      "     7869706       0.84      0.89      0.87        47\n",
      "     7869740       1.00      1.00      1.00        42\n",
      "     8508566       0.90      0.88      0.89        43\n",
      "     8508593       1.00      1.00      1.00        43\n",
      "     8761851       0.96      1.00      0.98        52\n",
      "     9073248       1.00      0.95      0.97        40\n",
      "     9239057       0.93      0.69      0.79        55\n",
      "     9239078       1.00      0.74      0.85        54\n",
      "     9239081       1.00      1.00      1.00        48\n",
      "     9239087       1.00      1.00      1.00        40\n",
      "     9239187       1.00      1.00      1.00        46\n",
      "     9240416       1.00      1.00      1.00        57\n",
      "     9240441       1.00      1.00      1.00        47\n",
      "     9577176       0.98      0.93      0.95        45\n",
      "     9577186       1.00      1.00      1.00        43\n",
      "     9577260       1.00      1.00      1.00        54\n",
      "     9577288       1.00      1.00      1.00        56\n",
      "     9577421       1.00      1.00      1.00        51\n",
      "     9577427       0.89      0.94      0.91        51\n",
      "     9577471       0.96      1.00      0.98        51\n",
      "    10062599       1.00      0.92      0.96        49\n",
      "    10243236       1.00      1.00      1.00        41\n",
      "    10243260       1.00      1.00      1.00        49\n",
      "    10243281       1.00      1.00      1.00        46\n",
      "    10243287       1.00      1.00      1.00        49\n",
      "    10482230       1.00      0.88      0.94        52\n",
      "    12388540       1.00      1.00      1.00        54\n",
      "    12388555       1.00      1.00      1.00        48\n",
      "    12388594       1.00      1.00      1.00        53\n",
      "    20000000       1.00      1.00      1.00        54\n",
      "    20000001       1.00      1.00      1.00        47\n",
      "    20000002       0.98      0.89      0.93        53\n",
      "    20000003       1.00      1.00      1.00        49\n",
      "    20000004       1.00      1.00      1.00        47\n",
      "    20000005       1.00      1.00      1.00        44\n",
      "    20000006       1.00      1.00      1.00        45\n",
      "    20000007       1.00      1.00      1.00        44\n",
      "    20000008       1.00      1.00      1.00        43\n",
      "    20000009       1.00      1.00      1.00        58\n",
      "    20000010       1.00      0.93      0.97        45\n",
      "    20000011       0.94      1.00      0.97        45\n",
      "    20000012       1.00      1.00      1.00        45\n",
      "    20000013       1.00      1.00      1.00        55\n",
      "    20000014       1.00      1.00      1.00        53\n",
      "    20000015       1.00      1.00      1.00        49\n",
      "    20000016       1.00      1.00      1.00        40\n",
      "    20000017       1.00      1.00      1.00        43\n",
      "    20000018       1.00      1.00      1.00        53\n",
      "    20000019       1.00      1.00      1.00        56\n",
      "    20000020       1.00      1.00      1.00        47\n",
      "    20000021       1.00      1.00      1.00        53\n",
      "    20000022       1.00      1.00      1.00        53\n",
      "    20000023       1.00      1.00      1.00        52\n",
      "    20000024       0.96      1.00      0.98        45\n",
      "    20000025       1.00      1.00      1.00        47\n",
      "    20000026       1.00      1.00      1.00        48\n",
      "    20000027       1.00      1.00      1.00        49\n",
      "    20000028       1.00      1.00      1.00        45\n",
      "    20000029       1.00      1.00      1.00        44\n",
      "    20000030       1.00      1.00      1.00        50\n",
      "    20000031       1.00      1.00      1.00        51\n",
      "    20000032       1.00      1.00      1.00        44\n",
      "    20000033       1.00      1.00      1.00        48\n",
      "    20000034       1.00      1.00      1.00        43\n",
      "    20000035       0.96      1.00      0.98        43\n",
      "    20000036       1.00      1.00      1.00        48\n",
      "    20000037       1.00      1.00      1.00        52\n",
      "    20000038       1.00      1.00      1.00        51\n",
      "    20000039       1.00      1.00      1.00        51\n",
      "    20000040       1.00      1.00      1.00        39\n",
      "    20000041       1.00      1.00      1.00        42\n",
      "    20000042       1.00      1.00      1.00        43\n",
      "    20000043       1.00      1.00      1.00        47\n",
      "    20000044       1.00      1.00      1.00        55\n",
      "    20000045       1.00      1.00      1.00        48\n",
      "    20000046       1.00      1.00      1.00        48\n",
      "    20000047       0.85      1.00      0.92        45\n",
      "    20000048       0.96      0.92      0.94        50\n",
      "    20000049       1.00      0.87      0.93        45\n",
      "    20000050       1.00      0.98      0.99        57\n",
      "    20000051       1.00      1.00      1.00        39\n",
      "    20000052       1.00      1.00      1.00        42\n",
      "    20000053       1.00      1.00      1.00        45\n",
      "    20000054       1.00      1.00      1.00        44\n",
      "    20000055       0.85      0.98      0.91        41\n",
      "    20000056       0.90      1.00      0.95        45\n",
      "    20000057       0.92      0.89      0.91        54\n",
      "    20000058       1.00      1.00      1.00        51\n",
      "    20000059       0.94      0.96      0.95        49\n",
      "    20000060       1.00      1.00      1.00        41\n",
      "    20000061       1.00      1.00      1.00        48\n",
      "    20000062       0.95      1.00      0.97        53\n",
      "    20000063       1.00      0.92      0.96        49\n",
      "    20000064       1.00      0.91      0.95        55\n",
      "    20000065       0.94      0.88      0.91        50\n",
      "    20000066       0.98      1.00      0.99        59\n",
      "    20000067       0.88      0.98      0.92        43\n",
      "    20000068       0.98      1.00      0.99        45\n",
      "    20000069       1.00      1.00      1.00        45\n",
      "    20000070       1.00      1.00      1.00        44\n",
      "    20000071       1.00      1.00      1.00        51\n",
      "    20000072       0.98      0.93      0.95        43\n",
      "    20000073       1.00      1.00      1.00        51\n",
      "    20000074       0.90      0.90      0.90        51\n",
      "    20000075       0.80      0.70      0.74        46\n",
      "    20000076       1.00      0.87      0.93        39\n",
      "    20000077       1.00      1.00      1.00        48\n",
      "    20000078       0.94      0.96      0.95        51\n",
      "    20000079       0.88      0.91      0.90        57\n",
      "    20000080       1.00      1.00      1.00        47\n",
      "    20000081       1.00      0.88      0.94        50\n",
      "    20000082       1.00      1.00      1.00        48\n",
      "    20000083       1.00      1.00      1.00        50\n",
      "    20000084       0.98      1.00      0.99        56\n",
      "    20000085       1.00      1.00      1.00        40\n",
      "    20000086       1.00      1.00      1.00        49\n",
      "    20000087       1.00      1.00      1.00        40\n",
      "    20000088       1.00      1.00      1.00        49\n",
      "    20000089       0.82      0.90      0.86        50\n",
      "    20000090       1.00      1.00      1.00        47\n",
      "    20000091       1.00      1.00      1.00        42\n",
      "    20000092       0.96      0.82      0.88        61\n",
      "    20000093       1.00      1.00      1.00        43\n",
      "    20000094       1.00      0.93      0.97        45\n",
      "    20000095       0.98      1.00      0.99        50\n",
      "    20000096       0.98      1.00      0.99        48\n",
      "    20000097       0.93      0.93      0.93        46\n",
      "    20000098       1.00      1.00      1.00        52\n",
      "    20000099       1.00      1.00      1.00        46\n",
      "    20000100       1.00      1.00      1.00        53\n",
      "    20000101       1.00      1.00      1.00        43\n",
      "    20000102       1.00      1.00      1.00        44\n",
      "    20000103       1.00      0.98      0.99        49\n",
      "    20000104       1.00      1.00      1.00        47\n",
      "    20000105       0.87      0.89      0.88        45\n",
      "    20000106       0.98      0.96      0.97        55\n",
      "    20000107       1.00      1.00      1.00        37\n",
      "    20000108       1.00      1.00      1.00        42\n",
      "    20000109       0.72      0.79      0.76        53\n",
      "    20000110       1.00      0.94      0.97        51\n",
      "    20000111       0.86      1.00      0.92        43\n",
      "    20000112       0.93      0.85      0.89        47\n",
      "    20000113       0.85      1.00      0.92        47\n",
      "    20000114       1.00      1.00      1.00        43\n",
      "    20000115       0.91      0.91      0.91        43\n",
      "    20000116       0.86      0.80      0.83        45\n",
      "    20000117       1.00      0.86      0.93        51\n",
      "    20000118       1.00      1.00      1.00        53\n",
      "    20000119       0.84      0.86      0.85        42\n",
      "    20000120       0.96      1.00      0.98        46\n",
      "    20000121       1.00      1.00      1.00        53\n",
      "    20000122       1.00      1.00      1.00        33\n",
      "    20000123       1.00      1.00      1.00        52\n",
      "    20000124       1.00      1.00      1.00        53\n",
      "    20000125       0.96      1.00      0.98        45\n",
      "    20000126       1.00      1.00      1.00        44\n",
      "    20000127       0.84      0.98      0.90        42\n",
      "    20000128       1.00      1.00      1.00        56\n",
      "    20000129       1.00      1.00      1.00        53\n",
      "    20000130       1.00      1.00      1.00        33\n",
      "    20000131       0.98      0.89      0.93        53\n",
      "    20000132       1.00      1.00      1.00        44\n",
      "    20000133       1.00      1.00      1.00        56\n",
      "    20000134       0.85      0.98      0.91        46\n",
      "    20000135       1.00      1.00      1.00        43\n",
      "    20000136       1.00      1.00      1.00        39\n",
      "    20000137       0.98      0.96      0.97        46\n",
      "    20000138       0.77      0.96      0.85        51\n",
      "    20000139       1.00      0.88      0.94        50\n",
      "    20000140       0.96      1.00      0.98        49\n",
      "    20000141       0.94      1.00      0.97        47\n",
      "    20000142       1.00      1.00      1.00        48\n",
      "    20000143       0.90      0.88      0.89        43\n",
      "    20000144       1.00      1.00      1.00        46\n",
      "    20000145       0.95      1.00      0.97        53\n",
      "    20000146       1.00      1.00      1.00        40\n",
      "    20000147       1.00      1.00      1.00        48\n",
      "    20000148       1.00      1.00      1.00        53\n",
      "    20000149       1.00      1.00      1.00        45\n",
      "    20000150       0.98      1.00      0.99        53\n",
      "    20000151       1.00      0.94      0.97        48\n",
      "    20000152       0.94      0.98      0.96        48\n",
      "    20000153       1.00      1.00      1.00        47\n",
      "    20000154       0.98      1.00      0.99        46\n",
      "    20000155       0.95      0.96      0.95        54\n",
      "    20000156       0.98      0.96      0.97        51\n",
      "    20000157       0.96      1.00      0.98        46\n",
      "    20000158       0.96      0.92      0.94        51\n",
      "    20000159       1.00      1.00      1.00        43\n",
      "    20000160       1.00      1.00      1.00        50\n",
      "    20000161       0.98      1.00      0.99        41\n",
      "    20000162       0.93      0.98      0.95        43\n",
      "    20000163       0.88      0.98      0.93        46\n",
      "    20000164       1.00      0.90      0.95        50\n",
      "    20000165       1.00      1.00      1.00        50\n",
      "    20000166       1.00      1.00      1.00        42\n",
      "    20000167       1.00      1.00      1.00        52\n",
      "    20000168       0.95      1.00      0.97        54\n",
      "    20000169       1.00      1.00      1.00        48\n",
      "    20000170       0.90      0.96      0.93        56\n",
      "    20000171       1.00      1.00      1.00        45\n",
      "    20000172       0.96      1.00      0.98        43\n",
      "    20000173       0.98      0.96      0.97        55\n",
      "    20000174       1.00      0.88      0.94        52\n",
      "    20000175       1.00      1.00      1.00        49\n",
      "    20000176       1.00      1.00      1.00        47\n",
      "    20000177       0.95      1.00      0.97        53\n",
      "    20000178       1.00      1.00      1.00        51\n",
      "    20000179       1.00      0.94      0.97        51\n",
      "    20000180       0.98      1.00      0.99        50\n",
      "    20000181       1.00      1.00      1.00        44\n",
      "    20000182       1.00      1.00      1.00        50\n",
      "    20000183       0.89      1.00      0.94        39\n",
      "    20000184       1.00      1.00      1.00        43\n",
      "    20000185       1.00      1.00      1.00        45\n",
      "    20000186       1.00      0.89      0.94        55\n",
      "    20000187       1.00      0.91      0.95        46\n",
      "    20000188       1.00      1.00      1.00        48\n",
      "    20000189       1.00      1.00      1.00        41\n",
      "    20000190       0.87      1.00      0.93        48\n",
      "    20000191       1.00      1.00      1.00        49\n",
      "    20000192       1.00      1.00      1.00        47\n",
      "    20000193       1.00      1.00      1.00        47\n",
      "    20000194       1.00      1.00      1.00        52\n",
      "    20000195       1.00      1.00      1.00        53\n",
      "    20000196       1.00      1.00      1.00        49\n",
      "    20000197       1.00      1.00      1.00        47\n",
      "    20000198       1.00      1.00      1.00        46\n",
      "    20000199       1.00      1.00      1.00        50\n",
      "    20000200       1.00      1.00      1.00        51\n",
      "    20000201       1.00      1.00      1.00        49\n",
      "    20000202       1.00      1.00      1.00        45\n",
      "    20000203       1.00      1.00      1.00        52\n",
      "    20000204       1.00      1.00      1.00        43\n",
      "    20000205       1.00      1.00      1.00        39\n",
      "    20000206       1.00      1.00      1.00        53\n",
      "    20000207       1.00      1.00      1.00        49\n",
      "    20000208       1.00      1.00      1.00        53\n",
      "    20000209       1.00      1.00      1.00        50\n",
      "    20000210       1.00      1.00      1.00        43\n",
      "    20000211       1.00      1.00      1.00        45\n",
      "    20000212       1.00      0.98      0.99        53\n",
      "    20000213       1.00      1.00      1.00        55\n",
      "    20000214       1.00      1.00      1.00        50\n",
      "    20000215       1.00      1.00      1.00        41\n",
      "    20000216       0.98      1.00      0.99        44\n",
      "    20000217       1.00      1.00      1.00        55\n",
      "    20000218       1.00      1.00      1.00        50\n",
      "    20000219       1.00      1.00      1.00        43\n",
      "    20000220       1.00      1.00      1.00        54\n",
      "    20000221       1.00      1.00      1.00        48\n",
      "    20000222       1.00      1.00      1.00        49\n",
      "    20000223       1.00      1.00      1.00        55\n",
      "    20000224       1.00      1.00      1.00        49\n",
      "    20000225       1.00      1.00      1.00        51\n",
      "    20000226       1.00      1.00      1.00        56\n",
      "    20000227       1.00      1.00      1.00        54\n",
      "    20000228       1.00      1.00      1.00        54\n",
      "    20000229       1.00      1.00      1.00        43\n",
      "    20000230       1.00      1.00      1.00        43\n",
      "    20000231       1.00      1.00      1.00        57\n",
      "    20000232       1.00      0.91      0.95        53\n",
      "    20000233       0.92      1.00      0.96        47\n",
      "    20000234       1.00      1.00      1.00        55\n",
      "    20000235       1.00      1.00      1.00        49\n",
      "    20000236       1.00      1.00      1.00        52\n",
      "    20000237       1.00      1.00      1.00        48\n",
      "    20000238       1.00      1.00      1.00        50\n",
      "    20000239       1.00      1.00      1.00        47\n",
      "    20000240       1.00      1.00      1.00        51\n",
      "    20000241       1.00      1.00      1.00        52\n",
      "    20000242       1.00      1.00      1.00        56\n",
      "    20000243       1.00      1.00      1.00        39\n",
      "    20000244       1.00      1.00      1.00        44\n",
      "    20000245       1.00      1.00      1.00        52\n",
      "    20000246       1.00      1.00      1.00        59\n",
      "    20000247       1.00      1.00      1.00        50\n",
      "    20000248       1.00      1.00      1.00        52\n",
      "    20000249       1.00      1.00      1.00        55\n",
      "    20000250       1.00      1.00      1.00        48\n",
      "    20000251       1.00      1.00      1.00        57\n",
      "    20000252       1.00      1.00      1.00        47\n",
      "    20000253       1.00      1.00      1.00        49\n",
      "    20000254       1.00      1.00      1.00        57\n",
      "    20000255       1.00      1.00      1.00        54\n",
      "    20000256       1.00      1.00      1.00        49\n",
      "    20000257       1.00      1.00      1.00        45\n",
      "    20000258       1.00      1.00      1.00        44\n",
      "    20000259       1.00      1.00      1.00        48\n",
      "    20000260       1.00      1.00      1.00        42\n",
      "    20000261       1.00      1.00      1.00        56\n",
      "    20000262       1.00      1.00      1.00        52\n",
      "    20000263       1.00      1.00      1.00        54\n",
      "    20000264       1.00      1.00      1.00        49\n",
      "    20000265       1.00      1.00      1.00        53\n",
      "    20000266       1.00      1.00      1.00        45\n",
      "    20000267       1.00      1.00      1.00        52\n",
      "    20000268       1.00      1.00      1.00        53\n",
      "    20000269       1.00      1.00      1.00        43\n",
      "    20000270       1.00      1.00      1.00        51\n",
      "    20000271       1.00      1.00      1.00        46\n",
      "    20000272       1.00      1.00      1.00        45\n",
      "    20000273       1.00      1.00      1.00        53\n",
      "    20000274       1.00      1.00      1.00        49\n",
      "    20000275       1.00      1.00      1.00        55\n",
      "    20000276       1.00      1.00      1.00        50\n",
      "    20000277       1.00      1.00      1.00        46\n",
      "    20000278       1.00      1.00      1.00        49\n",
      "    20000279       1.00      1.00      1.00        43\n",
      "    20000280       1.00      1.00      1.00        50\n",
      "    20000281       1.00      1.00      1.00        49\n",
      "    20000282       1.00      1.00      1.00        45\n",
      "    20000283       1.00      1.00      1.00        53\n",
      "    20000284       1.00      1.00      1.00        42\n",
      "    20000285       1.00      1.00      1.00        50\n",
      "    20000286       1.00      1.00      1.00        45\n",
      "    20000287       1.00      1.00      1.00        49\n",
      "    20000288       1.00      1.00      1.00        60\n",
      "    20000289       1.00      1.00      1.00        57\n",
      "    20000290       1.00      1.00      1.00        56\n",
      "    20000291       1.00      1.00      1.00        54\n",
      "    20000292       1.00      1.00      1.00        46\n",
      "    20000293       1.00      1.00      1.00        46\n",
      "    20000294       1.00      1.00      1.00        50\n",
      "    20000295       1.00      1.00      1.00        42\n",
      "    20000296       1.00      1.00      1.00        49\n",
      "    20000297       1.00      1.00      1.00        50\n",
      "    20000298       1.00      1.00      1.00        39\n",
      "    20000299       1.00      1.00      1.00        41\n",
      "    20000300       1.00      1.00      1.00        45\n",
      "    20000301       1.00      1.00      1.00        46\n",
      "    20000302       1.00      1.00      1.00        44\n",
      "    20000303       1.00      1.00      1.00        49\n",
      "    20000304       1.00      1.00      1.00        48\n",
      "    20000305       1.00      1.00      1.00        46\n",
      "    20000306       1.00      1.00      1.00        52\n",
      "    20000307       1.00      1.00      1.00        43\n",
      "    20000308       1.00      1.00      1.00        56\n",
      "    20000309       1.00      1.00      1.00        53\n",
      "    20000310       1.00      1.00      1.00        54\n",
      "    20000311       1.00      1.00      1.00        46\n",
      "    20000312       1.00      1.00      1.00        51\n",
      "    20000313       1.00      1.00      1.00        49\n",
      "    20000314       1.00      1.00      1.00        42\n",
      "    20000315       1.00      1.00      1.00        50\n",
      "    20000316       1.00      1.00      1.00        55\n",
      "    20000317       1.00      1.00      1.00        55\n",
      "    20000318       0.88      0.96      0.92        46\n",
      "    20000319       1.00      1.00      1.00        55\n",
      "    20000320       1.00      1.00      1.00        49\n",
      "    20000321       1.00      1.00      1.00        50\n",
      "    20000322       1.00      1.00      1.00        45\n",
      "    20000323       1.00      1.00      1.00        54\n",
      "    20000324       1.00      1.00      1.00        53\n",
      "    20000325       0.95      1.00      0.98        59\n",
      "    20000326       1.00      1.00      1.00        46\n",
      "    20000327       1.00      1.00      1.00        49\n",
      "    20000328       1.00      1.00      1.00        51\n",
      "    20000329       1.00      1.00      1.00        53\n",
      "    20000330       1.00      1.00      1.00        38\n",
      "    20000331       1.00      1.00      1.00        49\n",
      "    20000332       1.00      1.00      1.00        54\n",
      "    20000333       1.00      1.00      1.00        46\n",
      "    20000334       1.00      1.00      1.00        54\n",
      "    20000335       1.00      1.00      1.00        56\n",
      "    20000336       1.00      1.00      1.00        50\n",
      "    20000337       1.00      1.00      1.00        42\n",
      "    20000338       1.00      1.00      1.00        45\n",
      "    20000339       0.91      1.00      0.95        50\n",
      "    20000340       1.00      1.00      1.00        46\n",
      "    20000341       1.00      1.00      1.00        55\n",
      "    20000342       1.00      1.00      1.00        44\n",
      "    20000343       1.00      0.96      0.98        50\n",
      "    20000344       0.98      0.90      0.94        52\n",
      "    20000345       1.00      1.00      1.00        51\n",
      "    20000346       1.00      1.00      1.00        47\n",
      "    20000347       0.94      0.89      0.92        56\n",
      "    20000348       1.00      1.00      1.00        53\n",
      "    20000349       1.00      1.00      1.00        52\n",
      "    20000350       1.00      1.00      1.00        47\n",
      "    20000351       1.00      0.88      0.94        43\n",
      "    20000352       1.00      1.00      1.00        49\n",
      "    20000353       1.00      1.00      1.00        47\n",
      "    20000354       0.96      0.94      0.95        47\n",
      "    20000355       1.00      1.00      1.00        54\n",
      "    20000356       1.00      1.00      1.00        46\n",
      "    20000357       1.00      1.00      1.00        49\n",
      "    20000358       1.00      1.00      1.00        49\n",
      "    20000359       1.00      1.00      1.00        51\n",
      "    20000360       0.94      1.00      0.97        44\n",
      "    20000361       1.00      1.00      1.00        48\n",
      "    20000362       1.00      1.00      1.00        47\n",
      "    20000363       1.00      1.00      1.00        52\n",
      "    20000364       1.00      1.00      1.00        52\n",
      "    20000365       1.00      1.00      1.00        46\n",
      "    20000366       0.93      0.95      0.94        44\n",
      "    20000367       1.00      1.00      1.00        53\n",
      "    20000368       0.91      1.00      0.95        48\n",
      "    20000369       0.96      1.00      0.98        48\n",
      "    20000370       1.00      1.00      1.00        54\n",
      "    20000371       1.00      1.00      1.00        45\n",
      "    20000372       0.96      1.00      0.98        54\n",
      "    20000373       0.98      0.94      0.96        54\n",
      "    20000374       1.00      1.00      1.00        58\n",
      "    20000375       1.00      1.00      1.00        43\n",
      "    20000376       1.00      1.00      1.00        53\n",
      "    20000377       1.00      0.88      0.93        49\n",
      "    20000378       1.00      1.00      1.00        55\n",
      "    20000379       1.00      0.98      0.99        45\n",
      "    20000380       1.00      1.00      1.00        43\n",
      "    20000381       0.98      1.00      0.99        41\n",
      "    20000382       1.00      1.00      1.00        41\n",
      "    20000383       1.00      1.00      1.00        52\n",
      "    20000384       1.00      1.00      1.00        46\n",
      "    20000385       1.00      1.00      1.00        42\n",
      "    20000386       1.00      1.00      1.00        45\n",
      "    20000387       1.00      1.00      1.00        46\n",
      "    20000388       1.00      1.00      1.00        54\n",
      "    20000389       1.00      1.00      1.00        55\n",
      "    20000390       1.00      1.00      1.00        48\n",
      "    20000391       1.00      1.00      1.00        51\n",
      "    20000392       1.00      1.00      1.00        53\n",
      "    20000393       1.00      1.00      1.00        45\n",
      "    20000394       1.00      1.00      1.00        51\n",
      "    20000395       1.00      0.89      0.94        54\n",
      "    20000396       1.00      1.00      1.00        50\n",
      "    20000397       1.00      1.00      1.00        57\n",
      "    20000398       0.96      1.00      0.98        51\n",
      "    20000399       0.93      0.96      0.94        52\n",
      "    20000400       0.90      0.91      0.91        58\n",
      "    20000401       1.00      0.94      0.97        50\n",
      "    20000402       1.00      0.84      0.91        50\n",
      "    20000403       1.00      1.00      1.00        46\n",
      "    20000404       1.00      1.00      1.00        37\n",
      "    20000405       0.98      1.00      0.99        45\n",
      "    20000406       1.00      1.00      1.00        47\n",
      "    20000407       1.00      1.00      1.00        51\n",
      "    20000408       1.00      1.00      1.00        51\n",
      "    20000409       0.98      1.00      0.99        42\n",
      "    20000410       1.00      1.00      1.00        46\n",
      "    20000411       1.00      1.00      1.00        49\n",
      "    20000412       0.92      1.00      0.96        48\n",
      "    20000413       1.00      1.00      1.00        54\n",
      "    20000414       1.00      1.00      1.00        38\n",
      "    20000415       0.96      1.00      0.98        48\n",
      "    20000416       1.00      1.00      1.00        47\n",
      "    20000417       1.00      1.00      1.00        40\n",
      "    20000418       1.00      1.00      1.00        51\n",
      "    20000419       1.00      1.00      1.00        52\n",
      "    20000420       0.92      0.96      0.94        49\n",
      "    20000421       1.00      1.00      1.00        54\n",
      "    20000422       1.00      1.00      1.00        43\n",
      "    20000423       1.00      1.00      1.00        54\n",
      "    20000424       1.00      1.00      1.00        48\n",
      "    20000425       1.00      1.00      1.00        43\n",
      "    20000426       0.96      1.00      0.98        55\n",
      "    20000427       1.00      1.00      1.00        52\n",
      "    20000428       1.00      1.00      1.00        53\n",
      "    20000429       1.00      1.00      1.00        50\n",
      "    20000430       1.00      1.00      1.00        42\n",
      "    20000431       1.00      1.00      1.00        51\n",
      "    20000432       1.00      1.00      1.00        47\n",
      "    20000433       1.00      1.00      1.00        46\n",
      "    20000434       1.00      1.00      1.00        44\n",
      "    20000435       1.00      1.00      1.00        55\n",
      "    20000436       0.91      0.97      0.94        40\n",
      "    20000437       1.00      1.00      1.00        51\n",
      "    20000438       1.00      1.00      1.00        50\n",
      "    20000439       1.00      0.92      0.96        48\n",
      "    20000440       0.89      0.98      0.93        50\n",
      "    20000441       1.00      1.00      1.00        52\n",
      "    20000442       1.00      1.00      1.00        49\n",
      "    20000443       1.00      1.00      1.00        45\n",
      "    20000444       1.00      1.00      1.00        58\n",
      "    20000445       1.00      1.00      1.00        54\n",
      "    20000446       1.00      1.00      1.00        44\n",
      "    20000447       1.00      1.00      1.00        48\n",
      "    20000448       1.00      1.00      1.00        46\n",
      "    20000449       0.93      1.00      0.96        39\n",
      "    20000450       1.00      1.00      1.00        49\n",
      "    20000451       1.00      1.00      1.00        55\n",
      "    20000452       1.00      1.00      1.00        46\n",
      "    20000453       1.00      1.00      1.00        37\n",
      "    20000454       1.00      1.00      1.00        55\n",
      "    20000455       1.00      1.00      1.00        43\n",
      "    20000456       1.00      1.00      1.00        51\n",
      "    20000457       1.00      1.00      1.00        47\n",
      "    20000458       1.00      1.00      1.00        55\n",
      "    20000459       1.00      1.00      1.00        55\n",
      "    20000460       1.00      1.00      1.00        45\n",
      "    20000461       1.00      1.00      1.00        45\n",
      "    20000462       1.00      1.00      1.00        50\n",
      "    20000463       1.00      1.00      1.00        50\n",
      "    20000464       1.00      1.00      1.00        49\n",
      "    20000465       1.00      1.00      1.00        45\n",
      "    20000466       1.00      1.00      1.00        44\n",
      "    20000467       1.00      1.00      1.00        48\n",
      "    20000468       1.00      1.00      1.00        54\n",
      "    20000469       1.00      1.00      1.00        40\n",
      "    20000470       1.00      1.00      1.00        42\n",
      "    20000471       1.00      1.00      1.00        46\n",
      "    20000472       1.00      1.00      1.00        46\n",
      "    20000473       1.00      1.00      1.00        44\n",
      "    20000474       1.00      1.00      1.00        52\n",
      "    20000475       1.00      1.00      1.00        46\n",
      "    20000476       1.00      1.00      1.00        48\n",
      "    20000477       1.00      1.00      1.00        49\n",
      "    20000478       1.00      1.00      1.00        44\n",
      "    20000479       0.96      0.95      0.96        57\n",
      "    20000480       1.00      1.00      1.00        48\n",
      "    20000481       1.00      1.00      1.00        50\n",
      "    20000482       1.00      0.93      0.97        46\n",
      "    20000483       1.00      0.96      0.98        45\n",
      "    20000484       1.00      1.00      1.00        53\n",
      "    20000485       1.00      1.00      1.00        49\n",
      "    20000486       1.00      0.92      0.96        48\n",
      "    20000487       1.00      1.00      1.00        39\n",
      "    20000488       1.00      1.00      1.00        43\n",
      "    20000489       1.00      1.00      1.00        55\n",
      "    20000490       1.00      1.00      1.00        58\n",
      "    20000491       1.00      1.00      1.00        47\n",
      "    20000492       1.00      1.00      1.00        45\n",
      "    20000493       1.00      1.00      1.00        47\n",
      "    20000494       1.00      1.00      1.00        49\n",
      "    20000495       1.00      1.00      1.00        60\n",
      "    20000496       0.95      1.00      0.97        54\n",
      "    20000497       1.00      1.00      1.00        62\n",
      "    20000498       1.00      1.00      1.00        51\n",
      "    20000499       1.00      1.00      1.00        52\n",
      "    20000500       1.00      1.00      1.00        54\n",
      "    20000501       1.00      1.00      1.00        51\n",
      "    20000502       1.00      1.00      1.00        42\n",
      "    20000503       1.00      1.00      1.00        46\n",
      "    20000504       1.00      1.00      1.00        52\n",
      "    20000505       1.00      1.00      1.00        39\n",
      "    20000506       1.00      1.00      1.00        44\n",
      "    20000507       1.00      1.00      1.00        45\n",
      "    20000508       0.93      1.00      0.96        37\n",
      "    20000509       1.00      1.00      1.00        48\n",
      "    20000510       1.00      1.00      1.00        51\n",
      "    20000511       1.00      1.00      1.00        59\n",
      "    20000512       1.00      1.00      1.00        40\n",
      "    20000513       1.00      1.00      1.00        50\n",
      "    20000514       1.00      1.00      1.00        47\n",
      "    20000515       1.00      1.00      1.00        51\n",
      "    20000516       1.00      1.00      1.00        43\n",
      "    20000517       1.00      1.00      1.00        53\n",
      "    20000518       1.00      1.00      1.00        56\n",
      "    20000519       1.00      1.00      1.00        57\n",
      "    20000520       1.00      1.00      1.00        51\n",
      "    20000521       1.00      1.00      1.00        37\n",
      "    20000522       1.00      1.00      1.00        47\n",
      "    20000523       1.00      1.00      1.00        40\n",
      "    20000524       1.00      1.00      1.00        40\n",
      "    20000525       1.00      1.00      1.00        48\n",
      "    20000526       1.00      1.00      1.00        50\n",
      "    20000527       1.00      1.00      1.00        48\n",
      "    20000528       1.00      1.00      1.00        47\n",
      "    20000529       1.00      1.00      1.00        46\n",
      "    20000530       1.00      0.95      0.98        62\n",
      "    20000531       1.00      1.00      1.00        46\n",
      "    20000532       1.00      1.00      1.00        53\n",
      "    20000533       1.00      1.00      1.00        54\n",
      "    20000534       1.00      1.00      1.00        56\n",
      "    20000535       1.00      1.00      1.00        44\n",
      "    20000536       1.00      1.00      1.00        50\n",
      "    20000537       1.00      1.00      1.00        50\n",
      "    20000538       1.00      1.00      1.00        51\n",
      "    20000539       1.00      1.00      1.00        52\n",
      "    20000540       1.00      1.00      1.00        47\n",
      "    20000541       1.00      1.00      1.00        49\n",
      "    20000542       1.00      1.00      1.00        52\n",
      "    20000543       1.00      1.00      1.00        47\n",
      "    20000544       1.00      1.00      1.00        47\n",
      "    20000545       1.00      1.00      1.00        54\n",
      "    20000546       1.00      1.00      1.00        52\n",
      "    20000547       1.00      1.00      1.00        42\n",
      "    20000548       1.00      1.00      1.00        54\n",
      "    20000549       1.00      1.00      1.00        44\n",
      "    20000550       1.00      1.00      1.00        42\n",
      "    20000551       1.00      1.00      1.00        52\n",
      "    20000552       1.00      1.00      1.00        44\n",
      "    20000553       1.00      1.00      1.00        42\n",
      "    20000554       1.00      1.00      1.00        44\n",
      "    20000555       1.00      1.00      1.00        56\n",
      "    20000556       1.00      1.00      1.00        48\n",
      "    20000557       1.00      1.00      1.00        42\n",
      "    20000558       1.00      1.00      1.00        50\n",
      "    20000559       1.00      1.00      1.00        59\n",
      "    20000560       1.00      1.00      1.00        48\n",
      "    20000561       1.00      1.00      1.00        50\n",
      "    20000562       1.00      1.00      1.00        45\n",
      "    20000563       1.00      1.00      1.00        43\n",
      "    20000564       1.00      1.00      1.00        52\n",
      "    20000565       1.00      1.00      1.00        52\n",
      "    20000566       1.00      1.00      1.00        52\n",
      "    20000567       1.00      1.00      1.00        49\n",
      "    20000568       1.00      1.00      1.00        51\n",
      "    20000569       1.00      1.00      1.00        42\n",
      "    20000570       1.00      1.00      1.00        49\n",
      "    20000571       1.00      1.00      1.00        47\n",
      "    20000572       1.00      1.00      1.00        39\n",
      "    20000573       1.00      1.00      1.00        48\n",
      "    20000574       1.00      1.00      1.00        49\n",
      "    20000575       1.00      1.00      1.00        56\n",
      "    20000576       1.00      1.00      1.00        52\n",
      "    20000577       1.00      1.00      1.00        48\n",
      "    20000578       1.00      1.00      1.00        45\n",
      "    20000579       1.00      1.00      1.00        52\n",
      "    20000580       1.00      1.00      1.00        51\n",
      "    20000581       1.00      1.00      1.00        42\n",
      "    20000582       1.00      1.00      1.00        52\n",
      "    20000583       1.00      1.00      1.00        58\n",
      "    20000584       1.00      1.00      1.00        54\n",
      "    20000585       1.00      1.00      1.00        50\n",
      "    20000586       1.00      1.00      1.00        43\n",
      "    20000587       1.00      1.00      1.00        48\n",
      "    20000588       1.00      1.00      1.00        53\n",
      "    20000589       1.00      1.00      1.00        46\n",
      "\n",
      "    accuracy                           0.99     31283\n",
      "   macro avg       0.99      0.99      0.99     31283\n",
      "weighted avg       0.99      0.99      0.99     31283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "report = classification_report(test_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7be5b",
   "metadata": {},
   "source": [
    "#### Обучим модель CatBoostClassifier на подготовленных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a1443d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c000462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 5.8021328\ttest: 5.8028781\tbest: 5.8028781 (0)\ttotal: 4.36s\tremaining: 1h 12m 36s\n",
      "1:\tlearn: 5.7600391\ttest: 5.7615252\tbest: 5.7615252 (1)\ttotal: 8.73s\tremaining: 1h 12m 36s\n",
      "2:\tlearn: 5.7070225\ttest: 5.7088004\tbest: 5.7088004 (2)\ttotal: 13.3s\tremaining: 1h 13m 40s\n",
      "3:\tlearn: 5.6601069\ttest: 5.6619228\tbest: 5.6619228 (3)\ttotal: 17.8s\tremaining: 1h 13m 55s\n",
      "4:\tlearn: 5.6294886\ttest: 5.6320792\tbest: 5.6320792 (4)\ttotal: 22.2s\tremaining: 1h 13m 40s\n",
      "5:\tlearn: 5.5898019\ttest: 5.5922128\tbest: 5.5922128 (5)\ttotal: 26.7s\tremaining: 1h 13m 44s\n",
      "6:\tlearn: 5.5461853\ttest: 5.5487469\tbest: 5.5487469 (6)\ttotal: 31.3s\tremaining: 1h 13m 58s\n",
      "7:\tlearn: 5.5087435\ttest: 5.5118871\tbest: 5.5118871 (7)\ttotal: 35.8s\tremaining: 1h 14m 4s\n",
      "8:\tlearn: 5.4728678\ttest: 5.4764603\tbest: 5.4764603 (8)\ttotal: 40.5s\tremaining: 1h 14m 24s\n",
      "9:\tlearn: 5.4400767\ttest: 5.4432583\tbest: 5.4432583 (9)\ttotal: 45.3s\tremaining: 1h 14m 42s\n",
      "10:\tlearn: 5.3952072\ttest: 5.3984957\tbest: 5.3984957 (10)\ttotal: 50s\tremaining: 1h 14m 52s\n",
      "11:\tlearn: 5.3746788\ttest: 5.3780192\tbest: 5.3780192 (11)\ttotal: 54.8s\tremaining: 1h 15m 10s\n",
      "12:\tlearn: 5.3439429\ttest: 5.3478626\tbest: 5.3478626 (12)\ttotal: 59s\tremaining: 1h 14m 36s\n",
      "13:\tlearn: 5.3019149\ttest: 5.3059202\tbest: 5.3059202 (13)\ttotal: 1m 4s\tremaining: 1h 15m 38s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\qlever\\qlever_ml_classification\\TNVED_classification.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m grid_small \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m],\u001b[39m# 0.03, 0.09],\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdepth\u001b[39m\u001b[39m'\u001b[39m: [ \u001b[39m3\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m#'l2_leaf_reg': [1, 3, 5, 7, 9]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m grid_full \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.01\u001b[39m, \u001b[39m0.03\u001b[39m, \u001b[39m0.05\u001b[39m, \u001b[39m0.07\u001b[39m, \u001b[39m0.09\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdepth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m10\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m#'early_stopping_rounds': True,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39ml2_leaf_reg\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m9\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m grid_search_result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgrid_search(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         grid_small,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         X\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         y\u001b[39m=\u001b[39;49mtrain_labels,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/qlever/qlever_ml_classification/TNVED_classification.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n",
      "File \u001b[1;32mg:\\Apps\\anaconda3\\envs\\qlever\\Lib\\site-packages\\catboost\\core.py:4124\u001b[0m, in \u001b[0;36mCatBoost.grid_search\u001b[1;34m(self, param_grid, X, y, cv, partition_random_seed, calc_cv_statistics, search_by_train_test_split, refit, shuffle, stratified, train_size, verbose, plot, plot_file, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   4121\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(grid[key], Iterable):\n\u001b[0;32m   4122\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mParameter grid value is not iterable (key=\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m, value=\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(key, grid[key]))\n\u001b[1;32m-> 4124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tune_hyperparams(\n\u001b[0;32m   4125\u001b[0m     param_grid\u001b[39m=\u001b[39;49mparam_grid, X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, cv\u001b[39m=\u001b[39;49mcv, n_iter\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   4126\u001b[0m     partition_random_seed\u001b[39m=\u001b[39;49mpartition_random_seed, calc_cv_statistics\u001b[39m=\u001b[39;49mcalc_cv_statistics,\n\u001b[0;32m   4127\u001b[0m     search_by_train_test_split\u001b[39m=\u001b[39;49msearch_by_train_test_split, refit\u001b[39m=\u001b[39;49mrefit, shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   4128\u001b[0m     stratified\u001b[39m=\u001b[39;49mstratified, train_size\u001b[39m=\u001b[39;49mtrain_size, verbose\u001b[39m=\u001b[39;49mverbose, plot\u001b[39m=\u001b[39;49mplot, plot_file\u001b[39m=\u001b[39;49mplot_file,\n\u001b[0;32m   4129\u001b[0m     log_cout\u001b[39m=\u001b[39;49mlog_cout, log_cerr\u001b[39m=\u001b[39;49mlog_cerr,\n\u001b[0;32m   4130\u001b[0m )\n",
      "File \u001b[1;32mg:\\Apps\\anaconda3\\envs\\qlever\\Lib\\site-packages\\catboost\\core.py:4018\u001b[0m, in \u001b[0;36mCatBoost._tune_hyperparams\u001b[1;34m(self, param_grid, X, y, cv, n_iter, partition_random_seed, calc_cv_statistics, search_by_train_test_split, refit, shuffle, stratified, train_size, verbose, plot, plot_file, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   4015\u001b[0m     stratified \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(loss_function, STRING_TYPES) \u001b[39mand\u001b[39;00m is_cv_stratified_objective(loss_function)\n\u001b[0;32m   4017\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mHyperparameters search plot\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(params)]):\n\u001b[1;32m-> 4018\u001b[0m     cv_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_tune_hyperparams(\n\u001b[0;32m   4019\u001b[0m         param_grid, train_params[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_pool\u001b[39;49m\u001b[39m\"\u001b[39;49m], params, n_iter,\n\u001b[0;32m   4020\u001b[0m         fold_count, partition_random_seed, shuffle, stratified, train_size,\n\u001b[0;32m   4021\u001b[0m         search_by_train_test_split, calc_cv_statistics, custom_folds, verbose\n\u001b[0;32m   4022\u001b[0m     )\n\u001b[0;32m   4024\u001b[0m \u001b[39mif\u001b[39;00m refit:\n\u001b[0;32m   4025\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_fitted()\n",
      "File \u001b[1;32m_catboost.pyx:5128\u001b[0m, in \u001b[0;36m_catboost._CatBoost._tune_hyperparams\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5166\u001b[0m, in \u001b[0;36m_catboost._CatBoost._tune_hyperparams\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(loss_function='MultiClass', logging_level='Verbose')\n",
    "\n",
    "#Нужно контролить переобоучение, правильно выставить learning_rate\n",
    "\n",
    "grid_small = {'learning_rate': [0.01],# 0.03, 0.09],\n",
    "        'depth': [ 3]\n",
    "        #'l2_leaf_reg': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "\n",
    "\n",
    "grid_full = {'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.09],\n",
    "        'depth': [3, 4, 6, 7, 8, 9, 10],\n",
    "        #'early_stopping_rounds': True,\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "\n",
    "grid_search_result = model.grid_search(\n",
    "        grid_small,\n",
    "        X=train_data,\n",
    "        y=train_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels)\n",
    "preds_labels = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "25a403ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 4.4773927\ttotal: 27.6s\tremaining: 7h 40m 22s\n",
      "1:\tlearn: 4.1295189\ttotal: 54.8s\tremaining: 7h 35m 20s\n",
      "2:\tlearn: 3.5977834\ttotal: 1m 21s\tremaining: 7h 33m 10s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-b4b3f2764b55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Нужно контролить переобоучение, правильно выставить learning_rate, метрика и функция потерь\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpreds_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5098\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_function'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5100\u001b[1;33m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[0;32m   5101\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5102\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2317\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_cout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_cerr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2318\u001b[0m             \u001b[0mplot_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Training plots'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2319\u001b[1;33m             self._train(\n\u001b[0m\u001b[0;32m   2320\u001b[0m                 \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2321\u001b[0m                 \u001b[0mtrain_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eval_sets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1722\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1723\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1724\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_2 = CatBoostClassifier(loss_function='MultiClass', logging_level='Verbose', learning_rate = 0.3, depth = 4)\n",
    "\n",
    "#Нужно контролить переобоучение, правильно выставить learning_rate, метрика и функция потерь\n",
    "model_2.fit(train_data, train_labels)\n",
    "preds_labels = model_2.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1acf3d1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [18770, 437]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-81b4ed1a1848>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                     )\n\u001b[0;32m    210\u001b[0m                 ):\n\u001b[1;32m--> 211\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multilabel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_pred\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    408\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [18770, 437]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(test_labels, preds_labels)\n",
    "report = classification_report(test_labels, preds_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709817bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa65c4b",
   "metadata": {},
   "source": [
    "#### Удалим бесполезные факторы которые состоят из 1 уникального значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Делаем это после кодирования категориалььных переменных\n",
    "#for column in factors_df.columns:\n",
    "#    unique_values_count = factors_df[column].drop_duplicates().size\n",
    "#    if unique_values_count == 1:\n",
    "#        new_factors_df = factors_df.drop(column, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_factors_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_factors_df['Наименование терминального класса'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2902a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e819d83",
   "metadata": {},
   "source": [
    "* Удалить колонки для неполного соответствия: если есть [1,2,3,4] - удалить одну, чтобы не было зависимости\n",
    "* Удалить строки без характеристик\n",
    "* Отбор на основе важности признаков"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
